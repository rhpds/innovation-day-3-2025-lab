{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29946e6d-4745-441c-9614-4b584b59eef9",
   "metadata": {},
   "source": [
    "# Lab: Using LangChain to build a simple RAG pipeline\n",
    "\n",
    "## Goal\n",
    "\n",
    "Explore connection to models.corp, basic langchain and other component functionality with the ultimate goal of developing a simple **Retrieval Augmented Generation (RAG)** question and answer system based on publicly available Medium articles.\n",
    "\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Install dependencies\n",
    "2. Get models.corp credentials\n",
    "3. Quick langchain tutorial (optional)\n",
    "4. Download and transform data\n",
    "5. Generate embeddings and add to vector store\n",
    "6. Build the RAG chain\n",
    "7. Query and test the RAG system\n",
    "    \n",
    "---\n",
    "TO FINISH\n",
    "### Prerequisites\n",
    "\n",
    "#### Access to models.corp\n",
    "You need to .....\n",
    "VPN is required https://gitlab.cee.redhat.com/models-corp/user-documentation/-/blob/main/getting-started.md\n",
    "\n",
    "#### Python\n",
    "Python 3.13 is needed\n",
    "\n",
    "### Key components\n",
    "\n",
    "list the key components\n",
    "\n",
    "### And anything else\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5873d-3a82-455d-82e6-daab008dc4e7",
   "metadata": {},
   "source": [
    "## 1. Install dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b7641-091f-4573-b911-43a7398f5434",
   "metadata": {},
   "source": [
    "Uncomment the following cell and install the LangChain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "921dbf04-1026-4d69-9c7b-6d94ace8a324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Obtaining dependency information for langchain_community from https://files.pythonhosted.org/packages/5c/e1/975bcd11e86de74c10023d291879810d4eaffcfbb5d4c0d8fb6fb41b8247/langchain_community-0.3.25-py3-none-any.whl.metadata\n",
      "  Downloading langchain_community-0.3.25-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain_openai\n",
      "  Obtaining dependency information for langchain_openai from https://files.pythonhosted.org/packages/fc/9b/b8f86d78dbc651decd684ab938a1340e1ad3ba1dbcef805e12db65dee0ba/langchain_openai-0.3.24-py3-none-any.whl.metadata\n",
      "  Downloading langchain_openai-0.3.24-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain_milvus\n",
      "  Obtaining dependency information for langchain_milvus from https://files.pythonhosted.org/packages/3c/34/8daa975a92daff9cda1125b1ce9b280d3954e886fa76f07eb87cafde6b9a/langchain_milvus-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading langchain_milvus-0.2.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting langchain_text_splitters\n",
      "  Obtaining dependency information for langchain_text_splitters from https://files.pythonhosted.org/packages/8b/a3/3696ff2444658053c01b6b7443e761f28bb71217d82bb89137a978c5f66f/langchain_text_splitters-0.3.8-py3-none-any.whl.metadata\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langchain_huggingface\n",
      "  Obtaining dependency information for langchain_huggingface from https://files.pythonhosted.org/packages/c1/da/7446c2eeacd420cb975ceb49c6feca7be40cf8ed3686a128ca78410c148f/langchain_huggingface-0.3.0-py3-none-any.whl.metadata\n",
      "  Downloading langchain_huggingface-0.3.0-py3-none-any.whl.metadata (996 bytes)\n",
      "Collecting langchain_core\n",
      "  Obtaining dependency information for langchain_core from https://files.pythonhosted.org/packages/54/f0/31db18b7b8213266aed926ce89b5bdd84ccde7ee2edf4cab14e3dd2bfcf1/langchain_core-0.3.65-py3-none-any.whl.metadata\n",
      "  Downloading langchain_core-0.3.65-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain<1.0.0,>=0.3.25 (from langchain_community)\n",
      "  Obtaining dependency information for langchain<1.0.0,>=0.3.25 from https://files.pythonhosted.org/packages/ed/5c/5c0be747261e1f8129b875fa3bfea736bc5fe17652f9d5e15ca118571b6f/langchain-0.3.25-py3-none-any.whl.metadata\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain_community)\n",
      "  Obtaining dependency information for SQLAlchemy<3,>=1.4 from https://files.pythonhosted.org/packages/3c/35/f74add3978c20de6323fb11cb5162702670cc7a9420033befb43d8d5b7a4/sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_community) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_community) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
      "  Obtaining dependency information for aiohttp<4.0.0,>=3.8.3 from https://files.pythonhosted.org/packages/f3/9d/666d856cc3af3a62ae86393baa3074cc1d591a47d89dc3bf16f6eb2c8d32/aiohttp-3.12.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.12.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain_community)\n",
      "  Obtaining dependency information for tenacity!=8.4.0,<10,>=8.1.0 from https://files.pythonhosted.org/packages/e5/30/643397144bfbfec6f6ef821f36f33e57d35946c44a2352d3c9f0ae847619/tenacity-9.1.2-py3-none-any.whl.metadata\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Obtaining dependency information for dataclasses-json<0.7,>=0.5.7 from https://files.pythonhosted.org/packages/c3/be/d0d44e092656fe7a06b55e6103cbce807cdbdee17884a5367c68c9860853/dataclasses_json-0.6.7-py3-none-any.whl.metadata\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Obtaining dependency information for pydantic-settings<3.0.0,>=2.4.0 from https://files.pythonhosted.org/packages/b6/5f/d6d641b490fd3ec2c4c13b4244d68deea3a1b970a97be64f34fb5504ff72/pydantic_settings-2.9.1-py3-none-any.whl.metadata\n",
      "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting langsmith<0.4,>=0.1.125 (from langchain_community)\n",
      "  Obtaining dependency information for langsmith<0.4,>=0.1.125 from https://files.pythonhosted.org/packages/6a/f4/c206c0888f8a506404cb4f16ad89593bdc2f70cf00de26a1a0a7a76ad7a3/langsmith-0.3.45-py3-none-any.whl.metadata\n",
      "  Downloading langsmith-0.3.45-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Obtaining dependency information for httpx-sse<1.0.0,>=0.4.0 from https://files.pythonhosted.org/packages/e1/9b/a181f281f65d776426002f330c31849b86b31fc9d848db62e16f03ff739f/httpx_sse-0.4.0-py3-none-any.whl.metadata\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numpy>=1.26.2 (from langchain_community)\n",
      "  Obtaining dependency information for numpy>=1.26.2 from https://files.pythonhosted.org/packages/b7/f6/bc47f5fa666d5ff4145254f9e618d56e6a4ef9b874654ca74c19113bb538/numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting openai<2.0.0,>=1.86.0 (from langchain_openai)\n",
      "  Obtaining dependency information for openai<2.0.0,>=1.86.0 from https://files.pythonhosted.org/packages/f4/03/ef68d77a38dd383cbed7fc898857d394d5a8b0520a35f054e7fe05dc3ac1/openai-1.88.0-py3-none-any.whl.metadata\n",
      "  Downloading openai-1.88.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Obtaining dependency information for tiktoken<1,>=0.7 from https://files.pythonhosted.org/packages/1b/40/da42522018ca496432ffd02793c3a72a739ac04c3794a4914570c9bb2925/tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting pymilvus<3.0,>=2.5.7 (from langchain_milvus)\n",
      "  Obtaining dependency information for pymilvus<3.0,>=2.5.7 from https://files.pythonhosted.org/packages/e7/2c/a9f2c2daff511e127616a4294e597bf4c7626d49865f62865432698c7ba9/pymilvus-2.5.11-py3-none-any.whl.metadata\n",
      "  Downloading pymilvus-2.5.11-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tokenizers>=0.19.1 (from langchain_huggingface)\n",
      "  Obtaining dependency information for tokenizers>=0.19.1 from https://files.pythonhosted.org/packages/8a/63/38be071b0c8e06840bc6046991636bcb30c27f6bb1e670f4f4bc87cf49cc/tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting huggingface-hub>=0.30.2 (from langchain_huggingface)\n",
      "  Obtaining dependency information for huggingface-hub>=0.30.2 from https://files.pythonhosted.org/packages/33/fb/53587a89fbc00799e4179796f51b3ad713c5de6bb680b2becb6d37c94649/huggingface_hub-0.33.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain_core)\n",
      "  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain_core)\n",
      "  Obtaining dependency information for packaging<25,>=23.2 from https://files.pythonhosted.org/packages/88/ef/eb23f262cca3c0c4eb7ab1933c3b1f03d021f2c48f54763065b6f0e321be/packaging-24.2-py3-none-any.whl.metadata\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_core) (4.14.0)\n",
      "Collecting pydantic>=2.7.4 (from langchain_core)\n",
      "  Obtaining dependency information for pydantic>=2.7.4 from https://files.pythonhosted.org/packages/6a/c0/ec2b1c8712ca690e5d61979dee872603e92b8a32f94cc1b72d53beab008a/pydantic-2.11.7-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Obtaining dependency information for aiohappyeyeballs>=2.5.0 from https://files.pythonhosted.org/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Obtaining dependency information for aiosignal>=1.1.2 from https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dev/venv/lib64/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/8d/db/48421f62a6f77c553575201e89048e97198046b793f4a089c79a6e3268bd/frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Obtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/b6/b8/7a6e9c13c79709cdd2f22ee849f058e6da76892d141a67acc0e6c30d845c/multidict-6.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading multidict-6.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Obtaining dependency information for propcache>=0.2.0 from https://files.pythonhosted.org/packages/37/7c/54fd5301ef38505ab235d98827207176a5c9b2aa61939b10a460ca53e123/propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Obtaining dependency information for yarl<2.0,>=1.17.0 from https://files.pythonhosted.org/packages/98/28/3ab7acc5b51f4434b181b0cee8f1f4b77a65919700a355fb3617f9488874/yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/34/75/51952c7b2d3873b44a0028b1bd26a25078c18f92f256608e8d1dc61b39fd/marshmallow-3.26.1-py3-none-any.whl.metadata\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.30.2->langchain_huggingface)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/4d/36/2a115987e2d8c300a974597416d9de88f2444426de9571f4b59b2cca3acc/filelock-3.18.0-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.30.2->langchain_huggingface)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/bb/61/78c7b3851add1481b048b5fdc29067397a1784e2910592bc81bb3f608635/fsspec-2025.5.1-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.30.2->langchain_huggingface)\n",
      "  Obtaining dependency information for tqdm>=4.42.1 from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.30.2->langchain_huggingface)\n",
      "  Obtaining dependency information for hf-xet<2.0.0,>=1.1.2 from https://files.pythonhosted.org/packages/d1/a8/49a81d4f81b0d21cc758b6fca3880a85ca0d209e8425c8b3a6ef694881ca/hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/dev/venv/lib64/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/dev/venv/lib64/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.125->langchain_community)\n",
      "  Obtaining dependency information for orjson<4.0.0,>=3.9.14 from https://files.pythonhosted.org/packages/27/6f/875e8e282105350b9a5341c0222a13419758545ae32ad6e0fcf5f64d76aa/orjson-3.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading orjson-3.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.125->langchain_community)\n",
      "  Obtaining dependency information for requests-toolbelt<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/3f/51/d4db610ef29373b879047326cbf6fa98b6c1969d6f6dc423279de2b1be2c/requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.125->langchain_community)\n",
      "  Obtaining dependency information for zstandard<0.24.0,>=0.23.0 from https://files.pythonhosted.org/packages/fc/79/edeb217c57fe1bf16d890aa91a1c2c96b28c07b46afed54a5dcf310c3f6f/zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/dev/venv/lib64/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.86.0->langchain_openai)\n",
      "  Obtaining dependency information for distro<2,>=1.7.0 from https://files.pythonhosted.org/packages/12/b3/231ffd4ab1fc9d679809f356cebee130ac7daa00d6d6f3206dd4fd137e9e/distro-1.9.0-py3-none-any.whl.metadata\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.86.0->langchain_openai)\n",
      "  Obtaining dependency information for jiter<1,>=0.4.0 from https://files.pythonhosted.org/packages/e2/ba/77013b0b8ba904bf3762f11e0129b8928bff7f978a81838dfcc958ad5728/jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /home/dev/venv/lib64/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.7.4->langchain_core)\n",
      "  Obtaining dependency information for annotated-types>=0.6.0 from https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.7.4->langchain_core)\n",
      "  Obtaining dependency information for pydantic-core==2.33.2 from https://files.pythonhosted.org/packages/f9/41/4b043778cf9c4285d59742281a769eac371b9e47e35f98ad321349cc5d61/pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.7.4->langchain_core)\n",
      "  Obtaining dependency information for typing-inspection>=0.4.0 from https://files.pythonhosted.org/packages/17/69/cd203477f944c353c31bade965f880aa1061fd6bf05ded0726ca845b6ff7/typing_inspection-0.4.1-py3-none-any.whl.metadata\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
      "  Obtaining dependency information for python-dotenv>=0.21.0 from https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl.metadata\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: setuptools>69 in /home/dev/venv/lib64/python3.12/site-packages (from pymilvus<3.0,>=2.5.7->langchain_milvus) (80.9.0)\n",
      "Collecting grpcio<=1.67.1,>=1.49.1 (from pymilvus<3.0,>=2.5.7->langchain_milvus)\n",
      "  Obtaining dependency information for grpcio<=1.67.1,>=1.49.1 from https://files.pythonhosted.org/packages/92/cf/1d4c3e93efa93223e06a5c83ac27e32935f998bc368e276ef858b8883154/grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting protobuf>=3.20.0 (from pymilvus<3.0,>=2.5.7->langchain_milvus)\n",
      "  Obtaining dependency information for protobuf>=3.20.0 from https://files.pythonhosted.org/packages/fa/b1/b59d405d64d31999244643d88c45c8241c58f17cc887e73bcb90602327f8/protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting ujson>=2.0.0 (from pymilvus<3.0,>=2.5.7->langchain_milvus)\n",
      "  Obtaining dependency information for ujson>=2.0.0 from https://files.pythonhosted.org/packages/04/81/668707e5f2177791869b624be4c06fb2473bf97ee33296b18d1cf3092af7/ujson-5.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading ujson-5.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
      "Collecting pandas>=1.2.4 (from pymilvus<3.0,>=2.5.7->langchain_milvus)\n",
      "  Obtaining dependency information for pandas>=1.2.4 from https://files.pythonhosted.org/packages/01/a5/931fc3ad333d9d87b10107d948d757d67ebcfc33b1988d5faccc39c6845c/pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting milvus-lite>=2.4.0 (from pymilvus<3.0,>=2.5.7->langchain_milvus)\n",
      "  Obtaining dependency information for milvus-lite>=2.4.0 from https://files.pythonhosted.org/packages/44/43/b3f6e9defd1f3927b972beac7abe3d5b4a3bdb287e3bad69618e2e76cf0a/milvus_lite-2.4.12-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading milvus_lite-2.4.12-py3-none-manylinux2014_x86_64.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/dev/venv/lib64/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dev/venv/lib64/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dev/venv/lib64/python3.12/site-packages (from requests<3,>=2->langchain_community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dev/venv/lib64/python3.12/site-packages (from requests<3,>=2->langchain_community) (2025.6.15)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain_community)\n",
      "  Obtaining dependency information for greenlet>=1 from https://files.pythonhosted.org/packages/f6/f6/c82ac1851c60851302d8581680573245c8fc300253fc1ff741ae74a6c24d/greenlet-3.2.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading greenlet-3.2.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain_openai)\n",
      "  Obtaining dependency information for regex>=2022.1.18 from https://files.pythonhosted.org/packages/fb/13/e3b075031a738c9598c51cfbc4c7879e26729c53aa9cca59211c44235314/regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: httpcore==1.* in /home/dev/venv/lib64/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/dev/venv/lib64/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/dev/venv/lib64/python3.12/site-packages (from pandas>=1.2.4->pymilvus<3.0,>=2.5.7->langchain_milvus) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2.4->pymilvus<3.0,>=2.5.7->langchain_milvus)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2.4->pymilvus<3.0,>=2.5.7->langchain_milvus)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Obtaining dependency information for mypy-extensions>=0.3.0 from https://files.pythonhosted.org/packages/79/7b/2c79738432f5c924bef5071f933bcc9efd0473bac3b4aa584a6f7c1c8df8/mypy_extensions-1.1.0-py3-none-any.whl.metadata\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/dev/venv/lib64/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus<3.0,>=2.5.7->langchain_milvus) (1.17.0)\n",
      "Downloading langchain_community-0.3.25-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.3.24-py3-none-any.whl (68 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_milvus-0.2.0-py3-none-any.whl (35 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langchain_huggingface-0.3.0-py3-none-any.whl (27 kB)\n",
      "Downloading langchain_core-0.3.65-py3-none-any.whl (438 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.13-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m140.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.8/514.8 kB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.3.45-py3-none-any.whl (363 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m132.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.88.0-py3-none-any.whl (734 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.3/734.3 kB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m144.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pymilvus-2.5.11-py3-none-any.whl (228 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.1/228.1 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m145.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m139.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.2.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (605 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.5/605.5 kB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m164.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m162.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.0/352.0 kB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading milvus_lite-2.4.12-py3-none-manylinux2014_x86_64.whl (45.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (237 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.4/237.4 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m156.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading ujson-5.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.6/355.6 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m168.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, zstandard, ujson, tzdata, typing-inspection, tqdm, tenacity, regex, python-dotenv, pydantic-core, protobuf, propcache, packaging, orjson, numpy, mypy-extensions, multidict, jsonpatch, jiter, httpx-sse, hf-xet, grpcio, greenlet, fsspec, frozenlist, filelock, distro, annotated-types, aiohappyeyeballs, yarl, typing-inspect, tiktoken, SQLAlchemy, requests-toolbelt, pydantic, pandas, milvus-lite, marshmallow, huggingface-hub, aiosignal, tokenizers, pymilvus, pydantic-settings, openai, langsmith, dataclasses-json, aiohttp, langchain_core, langchain_text_splitters, langchain_openai, langchain_milvus, langchain_huggingface, langchain, langchain_community\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "Successfully installed SQLAlchemy-2.0.41 aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 annotated-types-0.7.0 dataclasses-json-0.6.7 distro-1.9.0 filelock-3.18.0 frozenlist-1.7.0 fsspec-2025.5.1 greenlet-3.2.3 grpcio-1.67.1 hf-xet-1.1.4 httpx-sse-0.4.0 huggingface-hub-0.33.0 jiter-0.10.0 jsonpatch-1.33 langchain-0.3.25 langchain_community-0.3.25 langchain_core-0.3.65 langchain_huggingface-0.3.0 langchain_milvus-0.2.0 langchain_openai-0.3.24 langchain_text_splitters-0.3.8 langsmith-0.3.45 marshmallow-3.26.1 milvus-lite-2.4.12 multidict-6.5.0 mypy-extensions-1.1.0 numpy-2.3.0 openai-1.88.0 orjson-3.10.18 packaging-24.2 pandas-2.3.0 propcache-0.3.2 protobuf-6.31.1 pydantic-2.11.7 pydantic-core-2.33.2 pydantic-settings-2.9.1 pymilvus-2.5.11 python-dotenv-1.1.0 pytz-2025.2 regex-2024.11.6 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.9.0 tokenizers-0.21.1 tqdm-4.67.1 typing-inspect-0.9.0 typing-inspection-0.4.1 tzdata-2025.2 ujson-5.10.0 yarl-1.20.1 zstandard-0.23.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community langchain_openai langchain_milvus langchain_text_splitters langchain_huggingface langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ccb1c4-b995-4227-93b2-a3be8c3af100",
   "metadata": {},
   "source": [
    "Other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d191cc-1e6a-48f5-9c5c-bf80f2216e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install beautifulsoup4 python-dotenv chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc154dad-f317-4365-a483-f516078e1238",
   "metadata": {},
   "source": [
    "## 2. Get models.corp credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c437e-a492-4ca9-b9ab-e53c6a67e988",
   "metadata": {},
   "source": [
    "In this lab we will be using a **granite-3.1-8b-instruct** model. \n",
    "\n",
    "To connect to the model ou will need the model id, the model API url and your corresponding API key \n",
    "\n",
    "#### Model id and url\n",
    "The models.corp user documentation gives a list of models https://gitlab.cee.redhat.com/models-corp/user-documentation#open-model-list\n",
    "On the **granite-3.1-8b-instruct** page, you can find the model string (model id) and api (model API url) along with other information about the model.     https://gitlab.cee.redhat.com/models-corp/user-documentation/-/blob/main/models/granite-3-1-8b-instruct.md\n",
    "\n",
    "#### Model API key\n",
    "To gain 14 day access to models.corps you need to create an account and complete a ServiceNow request. These SNow tickets are automated, and you should have sandbox access within 30 minutes. https://gitlab.cee.redhat.com/models-corp/user-documentation/-/blob/main/getting-started.md  VPN needed\n",
    "\n",
    "Find your api key from the application tab on the models.corp platform https://developer.models.corp.redhat.com\n",
    "For this lab we will be using a granite-3.1-8b-instruct. Find the key corresponding to this model in the credentials column in the applications tab.\n",
    "\n",
    "**Storing your key**\n",
    "\n",
    "You will need to store this api key variable securely in your local environment (not in this notebook).\n",
    "Create a `.env` file at the same location as this notebook and insert the line `GRANITE_ACCESS_TOKEN = \"YOUR TOKEN GOES HERE\"` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b74e1c5-edc0-462e-9ebf-ca37e5d5aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# Access the access token\n",
    "# access_token = os.getenv(\"GRANITE_ACCESS_TOKEN\")\n",
    "access_token = \"dummy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "028d1b98-8b70-4bac-a93e-231fa16063a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the variables for the model id and url\n",
    "model=\"ibm-granite/granite-3.1-8b-instruct\"\n",
    "model_api_url=\"https://granite-3-1-8b-instruct--apicast-production.apps.int.stc.ai.prod.us-east-1.aws.paas.redhat.com/v1\"\n",
    "\n",
    "model = \"granite3.3\"\n",
    "model_api_url=\"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90498e32-35a2-447f-9f51-c78ce2dbbb57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### SSL certificate\n",
    "\n",
    "You may need to add an SSL certificate to connect to models.corp. You can find the PEM file in this folder. ADD LINK "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810f19c-3d98-4ddd-8ff7-fe07972618fb",
   "metadata": {},
   "source": [
    "**MAC or Linux**\n",
    "\n",
    "Add the PEM file to your current working directory and run the bash command:\n",
    "```bash\n",
    "cat 2022-IT-Root-CA.pem >> `python -m certifi`\n",
    "```\n",
    "\n",
    "**Windows**\n",
    "\n",
    "Add the PEM file to your current working directory and run the Powershell command:\n",
    "```powershell\n",
    "cat .\\2022-IT-Root-CA.pem >> $(python3 -m certifi)\n",
    "```\n",
    "\n",
    "Alternatively you can add the certificate path to your environmental variables `SSL_CERT_FILE=[Path to 2022-IT-Root-CA.pem ]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb0e9d-ecfa-4408-8c7c-1636540e36b0",
   "metadata": {},
   "source": [
    "## 3. Quick langchain tutorial (optional)\n",
    "\n",
    "*NOTE: This section does not contribute to the RAG system. It is included for testing and exploration.* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec59ba34-6eb6-4f4a-9ea8-27da68118113",
   "metadata": {},
   "source": [
    "LangChain is a powerful framework that simplifies the development of AI applications by providing a standard way to chain together language models with external data sources and other components and tools.\n",
    "\n",
    "In LangChain, components are the modular, reusable building blocks of an AI application, such as Models, Prompt Templates, and Output Parsers. These individual components are then linked together into chains, which define a complete workflow for a specific task, such as answering a question or summarizing a document. Using the LangChain Expression Language (LCEL), you can connect these components, creating an automated data flow from the initial user input to the final, processed output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d87f9d-fafd-4b86-b4ee-5516044c510e",
   "metadata": {},
   "source": [
    "### 3a. Models: the core engine\n",
    "A Model in LangChain is a wrapper around a large language model like Granite (the model we are using for this lab).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f15a20a-e610-47e4-972d-25812f4ade38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The official currency of Italy is the Euro. The Euro was introduced as a common currency for many European countries in 1999, and it replaced various national currencies, including the Italian Lira. As a member of the Eurozone, Italy adopted the Euro for cash transactions, and it is now used for all official financial dealings within the country. The Euro is also utilized by 19 out of the 27 European Union (EU) member states, facilitating trade and travel among these nations.\n"
     ]
    }
   ],
   "source": [
    "# minimum import needed\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Temperature loose guidelines: 0-0.4 LOW deterministic;  0.5-1.0  MEDIUM balanced creativey and coherence; 1.1-2 HIGH creative and random\n",
    "\n",
    "# Initialize the model. Set a low temperature for predictable, less creative responses\n",
    "testllm = ChatOpenAI(model=model, api_key=access_token, base_url=model_api_url, temperature=0.1)\n",
    "\n",
    "# We can now \"invoke\" the model with a simple prompt\n",
    "response = testllm.invoke(\"What is the official currency of Italy?\")\n",
    "\n",
    "# The response is an AI Message object, so we access its content\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354694d2-40be-412e-86e6-fccd49438f71",
   "metadata": {},
   "source": [
    "### 3b. Output Parsers: Structuring the Response\n",
    "\n",
    "The output from an LLM is typically an AI Message object. An output parser is a class that helps you structure the model's response into a more usable format, like a simple string, a list, or a JSON object.\n",
    "\n",
    "In the last exercise, we used `response.content` to get the content string from our model response. Here we will use an output parser component which we can use later in a LangChain chain. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acb518f2-c5e0-4283-bfd9-41841294ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Josephine Baker (1891-1975) was an American-born French entertainer and civil rights activist, celebrated for her dazzling performances in Parisian nightclubs. Known as \"The Black Venus,\" she broke racial barriers with her exotic dance routines, captivating audiences worldwide. Beyond the stage, Baker was a dedicated anti-colonial and civil rights campaigner, adopting 12 children of diverse backgrounds and fighting for equality. Her legacy endures as an icon of artistic brilliance and social justice.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# The llm.invoke() call returns an AIMessage object\n",
    "response = testllm.invoke(\"Give me a 60 word biography on Josephine Baker\")\n",
    "\n",
    "# The parser converts the AIMessage into a simple string\n",
    "parsed_output = output_parser.invoke(response)\n",
    "\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88814be-8041-4eac-80b7-b35f47b8df41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3c. Prompt Templates: Crafting Your Instructions\n",
    "A prompt template in LangChain is a reusable object that creates a complete and formatted prompt for a language model by dynamically inserting user inputs and other variables into a predefined text structure.\n",
    "\n",
    "We will compare two prompt template classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0001d-5cf0-4d30-8304-d97e8d2b6fc9",
   "metadata": {},
   "source": [
    "#### PromptTemplate\n",
    "This class creates a single string from a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "caaa5513-ddd9-4a83-9343-ec8dcabe28b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Translate the expression little by little from English to French.'\n",
      "Type: <class 'langchain_core.prompt_values.StringPromptValue'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define a PromptTemplate object with a placeholder for a word to translate\n",
    "PT_prompt = PromptTemplate(\n",
    "    template=\"Translate the expression {toTranslate} from English to French.\",\n",
    "    input_variables=[\"toTranslate\"]\n",
    ")\n",
    "\n",
    "#invoke the template and print the output and type \n",
    "formatted_PT_prompt=PT_prompt.invoke({\"toTranslate\": \"little by little\"})\n",
    "\n",
    "print(formatted_PT_prompt)\n",
    "print(f\"Type: {type(formatted_PT_prompt)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8f266-8799-4ad4-853d-62b8a41d3d3e",
   "metadata": {},
   "source": [
    "#### ChatPromptTemplate\n",
    "This class creates a structured list of messages for chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3c10c3a-994f-430f-aac1-e69b77d38fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Translate the expression little by little from English to French.', additional_kwargs={}, response_metadata={})]\n",
      "Type: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a ChatPromptTemplate object with a placeholder for a word to translate\n",
    "CPT_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the expression {toTranslate} from English to French.\"\n",
    ")\n",
    "\n",
    "#invoke the template and print the output and type \n",
    "formatted_CPT_prompt=CPT_prompt.invoke({\"toTranslate\": \"little by little\"})\n",
    "\n",
    "print(formatted_CPT_prompt)\n",
    "print(f\"Type: {type(formatted_CPT_prompt)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c17a9b3-81b0-4bb7-b973-64db34d35668",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "`ChatPromptTemplate` creates a structured list of messages whereas `PromptTemplate`creates a single string.\n",
    "\n",
    "`ChatPromptTemplate` is used more commonly because modern models are optimised for a sequence of messages. They perform best when they receive a structured list of messages with roles (System for instructions, Human for user input), and ChatPromptTemplate is designed specifically for this.\n",
    "However it is also possible to format those roles manually with`PromptTemplate` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9ac0d8-8d13-40bf-a8f3-a132108b3f4d",
   "metadata": {},
   "source": [
    "### 3d. LangChain Expression Language (LCEL): Chaining It All Together\n",
    "\n",
    "LCEL is the declarative syntax used to chain LangChain components together. It uses the pipe symbol ( | ). Data flows from one component to the next in the sequence.\n",
    "\n",
    "We will create a simple chain using use the `testllm` model, the `PT_Prompt` template, and the `output_parser` from the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e14bb11-d6f2-48a1-96b6-807bdb265e05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PT_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Build the chain\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m test_chain=\u001b[43mPT_prompt\u001b[49m | testllm | output_parser\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Invoke the chain with the input data \"congratulations on building this chain\" (the expression we want to translate into French)\u001b[39;00m\n\u001b[32m      5\u001b[39m chain_output = test_chain.invoke(\u001b[33m\"\u001b[39m\u001b[33mcongratulations on building this chain\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'PT_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "# Build the chain\n",
    "test_chain=PT_prompt | testllm | output_parser\n",
    "\n",
    "# Invoke the chain with the input data \"Congratulations on building this chain\" (the expression we want to translate into French)\n",
    "\n",
    "chain_output = test_chain.invoke(\"congratulations on building this chain\")\n",
    "\n",
    "print(chain_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfecd118-8298-4f8c-b4f5-3be1fc735f61",
   "metadata": {},
   "source": [
    "Congratulations on your first LangChain chain!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae264e-6b4f-4f77-8657-d817d4c9a14f",
   "metadata": {},
   "source": [
    "## 4. Download and transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4465c0d-1445-4c13-938c-902015484169",
   "metadata": {},
   "source": [
    "The example uses multiple Medium articles on the subject of generative AI as the source document.\n",
    "\n",
    "We will load the documents and split them into shorter document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30d7dd4-a4ba-43ec-98dc-053d70300afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Instantiate the WebBaseLoader to fetch content from specified URLs.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\n",
    "        \"https://medium.com/@tuhinsharma121/mastering-prompt-engineering-a-beginners-guide-to-ai-interaction-2a28434ccb67\",\n",
    "        \"https://medium.com/@rahuljangir2992/graph-based-prompting-revolutionizing-ai-reasoning-f316b7266c1f\",\n",
    "        \"https://medium.com/@fassha08/transforming-search-ai-agents-and-multi-vector-intelligence-1bde1dbe66e7\",\n",
    "        \"https://medium.com/@harshkumar1146/prompt-chaining-unlocking-the-full-potential-of-ai-assistants-4fdf2f28c1a5\",\n",
    "    ),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The load() method fetches and parses the content from the URLs returning a list of Document objects, where each object contains the text content of one webpage.\n",
    "documents = loader.load()\n",
    "\n",
    "# Initialize the RecursiveCharacterTextSplitter which will break down large texts into smaller chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9689c029-bf6f-4d27-99ae-085c557e0392",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Exploring the document chunks\n",
    "\n",
    "*Note: This does not contribute to building the RAG system*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58002b80-3ab7-446e-a09f-5b448f2b0bd3",
   "metadata": {},
   "source": [
    "The output, `docs`, is a flat list of all the text chunks derived from the original documents. These smaller, more granular pieces of text are now in an ideal format to be used for creating vector embeddings for a similarity search in a RAG pipeline.\n",
    "\n",
    "If we look at the first two list items we can see the included metadata and note the overlap in the `page_content` chunks. \n",
    "Without overlap, you risk cutting a sentence or a complete thought exactly in half, leading to two incoherent chunks that lose their meaning. This severely damages the ability of the RAG system to both find the right information (retrieval) and understand it correctly (generation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8730bfc9-45f7-4e2c-bfd5-73ca1079dd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The articles have been split into 38 sub-documents. \n",
      "\n",
      "First sub-document:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://medium.com/@tuhinsharma121/mastering-prompt-engineering-a-beginners-guide-to-ai-interaction-2a28434ccb67', 'title': 'Mastering Prompt Engineering: A Beginner’s Guide to AI Interaction | by Tuhin Sharma | Medium', 'description': 'In today’s world of artificial intelligence (AI), prompt engineering has become a key skill. It changes how we talk to AI models and make them work better. Whether you’re experienced or just…', 'language': 'en'}, page_content='Mastering Prompt Engineering: A Beginner’s Guide to AI Interaction | by Tuhin Sharma | MediumSitemapSign upSign inMedium LogoWriteSign upSign inMastering Prompt Engineering: A Beginner’s Guide to AI InteractionTuhin SharmaFollow11 min read·Jul 10, 2024--1ListenShareIn today’s world of artificial intelligence (AI), prompt engineering has become a key skill. It changes how we talk to AI models and make them work better. Whether you’re experienced or just starting, knowing how to create good prompts can make a big difference in how well your AI applications perform.Prompt engineering is about designing the questions or instructions we give to AI models. This helps them give accurate and useful answers. It’s not just a technical skill; it’s also about using creativity and careful thinking to get the best results from AI.In this blog, we’ll explore what prompt engineering is, how it works, and why it matters. You’ll learn simple techniques and see real-life examples of how it can improve things like chatbots and content creation. Join us as we uncover the basics of prompt engineering and how it can help you make smarter, more effective AI solutions.1. Zero Shot PromptingZero-shot prompting is a technique used in the field of artificial intelligence where a model generates responses or performs tasks without any prior specific training on those tasks. This approach relies on the general knowledge and capabilities that the model has acquired during its initial comprehensive training phase. In zero-shot prompting, the model is given a task description or a prompt and is expected to understand and execute the task based solely on this input, without any examples or additional guidance. This method is particularly valuable for testing the flexibility and adaptability of AI models, as it showcases their ability to handle a wide variety of tasks and questions they have not explicitly been prepared for.ExamplePrompt:Classify the text into neutral, negative or positive. Text: I')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"The articles have been split into {len(docs)} sub-documents. \\n\")\n",
    "print (\"First sub-document:\")\n",
    "docs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f07c757-e7ec-48d7-988d-3d75b1217fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second sub-document:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://medium.com/@tuhinsharma121/mastering-prompt-engineering-a-beginners-guide-to-ai-interaction-2a28434ccb67', 'title': 'Mastering Prompt Engineering: A Beginner’s Guide to AI Interaction | by Tuhin Sharma | Medium', 'description': 'In today’s world of artificial intelligence (AI), prompt engineering has become a key skill. It changes how we talk to AI models and make them work better. Whether you’re experienced or just…', 'language': 'en'}, page_content='as it showcases their ability to handle a wide variety of tasks and questions they have not explicitly been prepared for.ExamplePrompt:Classify the text into neutral, negative or positive. Text: I think the vacation is okay.Sentiment:Output:NeutralLimitationFirstly, these models often aren’t as accurate as models trained on specific tasks because they’re trying to generalize without direct examples. This can lead to errors or lower confidence in the results. Also, because they handle such a broad range of tasks, zero-shot models might struggle with very detailed or niche requests, where specialized knowledge is needed.Prompt:An example of a sentence that uses the word farduddle is:Output:The word \"farduddle\" doesn\\'t appear to be a standard or recognized English word. Could it be a typo or a specific term from a particular context or language that you\\'re exploring? If you have more details or a different spelling, please share, and I\\'ll be happy to help with a sentence!2. Few Shot PromptingFew-shot prompting is a technique used in the field of artificial intelligence, particularly with models like GPT (Generative Pre-trained Transformer), to improve the model’s ability to understand and generate context-specific responses with minimal input data. Unlike zero-shot or one-shot methods, few-shot prompting involves providing the AI with a few examples to guide its responses, thereby helping the model infer the desired task from these examples without explicit programming.ExamplePrompt:A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:We were traveling in Africa and we saw these very cute whatpus. To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:Output:When we won the game, we all started to farduddle in celebration.LimitationStandard few-shot prompting is good for many tasks, but it’s not perfect, especially for complex thinking tasks. Let’s')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Second sub-document:\")\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab10660-1d91-418a-b759-a1839feea4cc",
   "metadata": {},
   "source": [
    "## 5. Generate embeddings and add to vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b85fe-a5b2-4bfb-bea2-7b33fd4d5569",
   "metadata": {},
   "source": [
    "### 5a. Embeddings\n",
    "Vector embeddings are dense numerical representations of data (our article chunks in this lab). An embedding is essentially an array of numbers that can represent a vector in multidimensional space. These vector representations position similar concepts closer together within this high-dimensional vector space, creating a semantic map of the text.\n",
    "\n",
    "To generate embeddings from our text chunks we use an embedding model, mxbai-embed-large-v1, from HuggingFace. This model converts each text chunk into an array with 1024 dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927b7933-4dc7-4f97-a3f3-042d95891098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vhunt\\Documents\\Projects\\RAG-InnovationDays\\IDvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Specify the embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd85927-7865-40d9-988e-c6f6ac3d7977",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Exploring embeddings\n",
    "We can explore an embedding on some sample text.\n",
    "\n",
    "*Note: This does not contribute to building the RAG system*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f584e9dd-351e-48e7-b985-368c22f3258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dimensions 1024\n",
      "Data type of the numbers <class 'float'>\n",
      "Here are the first 5 dimensions (numbers) of the vector\n",
      "[0.1568078249692917, 0.26713573932647705, -0.11924576759338379, 0.4239087998867035, -1.244588851928711]\n"
     ]
    }
   ],
   "source": [
    "sample_text=\"What are the advantages of few shot prompting?\"\n",
    "\n",
    "vector_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "# Print some information about the vector\n",
    "print(f\"Number of dimensions {len(vector_embedding)}\")\n",
    "print(f\"Data type of the numbers {type(vector_embedding[0])}\")\n",
    "\n",
    "# Print the first few numbers to give a sense of what it looks like\n",
    "print(\"Here are the first 5 dimensions (numbers) of the vector\")\n",
    "print(vector_embedding[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596be2e2-9d70-477a-8bdc-bce63e0ec1a5",
   "metadata": {},
   "source": [
    "### 5b. Vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9678f9c-cf04-4ec9-b1ba-f21ae18941a0",
   "metadata": {},
   "source": [
    "We will add the embeddings and the document chunks to an in-memory vector database. \n",
    "\n",
    "Two options are presented below. Both Milvus and Chroma are vector databases that can operate in-memory to provide fast, low-latency similarity searches for AI applications. Milvus is built for very large, complex projects that need to handle huge amounts of data, while Open Source Chroma is designed to be very simple and easy to start with for smaller, single-computer applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb744cb3-92d2-4b15-8f26-50322a4d43b9",
   "metadata": {},
   "source": [
    "#### Option 1 - Milvus\n",
    "\n",
    "(will not work if you are on a Windows machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804775e7-a081-40dd-82c8-feec895b18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "\n",
    "vectorstore = Milvus.from_documents(  \n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    connection_args={\n",
    "        \"uri\": \"./milvus_demo.db\",\n",
    "    },\n",
    "    drop_old=True,  # Drop the old Milvus collection if it exists\n",
    "    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
    "\n",
    ")\n",
    "\n",
    "#check the number of entries corresponds with the number of sub-documents (chunks)\n",
    "vectorstore.col.flush()\n",
    "number_of_entries = vectorstore.col.num_entities\n",
    "print(f\"The number of entries in the vector store is: {number_of_entries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b1c56e-8e62-4d3e-a230-a6f82bab497f",
   "metadata": {},
   "source": [
    "#### Option 2 - Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d08239-950b-4014-b0ae-2198f4fb21b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to delete collection: 'Medium_articles'...\n",
      "Collection did not exist, so no deletion was needed.\n",
      "The vector store contains 38 entries.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "#from langchain_community.embeddings import FakeEmbeddings\n",
    "#import chromadb\n",
    "#client = chromadb.EphemeralClient()\n",
    "\n",
    "# Deletes the database entries if they already exist\n",
    "try:\n",
    "    print(f\"Attempting to delete collection: '{\"Medium_articles\"}'...\")\n",
    "    vectorstore.delete_collection()\n",
    "    print(\"Collection deleted successfully.\")\n",
    "except Exception as e:\n",
    "    # This error is expected on the very first run when the collection doesn't exist yet.\n",
    "    print(f\"Collection did not exist, so no deletion was needed.\")\n",
    "    \n",
    "\n",
    "# Adds the document chunks and the embeddings.\n",
    "vectorstore = Chroma.from_documents(\n",
    "    #client=client,\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"Medium_articles\",\n",
    "    collection_metadata={\"hnsw:space\": \"l2\"}\n",
    "    \n",
    ")\n",
    "\n",
    "#check the number of entries corresponds with the number of sub-documents (chunks)\n",
    "number_of_entries = vectorstore._collection.count()\n",
    "print(f\"The vector store contains {number_of_entries} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d4059-fb88-4b90-b82c-c20fe641bd92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### About index parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538617e8-9374-4f2c-bba7-a28ef80bdb34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "An index in a vector database is a data structure that organizes the vectors. \n",
    "FLAT and HNSW (Hierarchical Navigable Small World) are two different types of indexes used in vector databases to manage and search through high-dimensional data. They represent a fundamental trade-off between search accuracy and speed.\n",
    "\n",
    "**FLAT Index** \n",
    "\n",
    "A FLAT index is the most basic approach to vector search. It is a brute-force method where a query vector is directly compared to every single other vector in the dataset. It is slow, but 100% accurate.\n",
    "In our example above the Milvus instance uses a FLAT index. We can change this parameter setting to HNSW.\n",
    "\n",
    "\n",
    "**HNSW (Hierarchical Navigable Small World) Index** \n",
    "\n",
    "HNSW is a sophisticated graph-based index that provides a powerful balance between search speed and accuracy. It's an Approximate Nearest Neighbor (ANN) algorithm, meaning it finds results that are most likely the nearest neighbors, but without a 100% guarantee.\n",
    "In our example above the Chroma instance uses a HNSW index.\n",
    "\n",
    "**Similarity Measures**\n",
    "\n",
    "In both cases we have specified L2 as an index parameter.\n",
    "\n",
    "This parameter defines how similarity between two vector embeddings is measured. \n",
    "L2 signifies that similarity between 2 vectors is measured by the shortest Euclidean distance between them in multi-dimensional space. \n",
    "\n",
    "In simpler terms, the vector embeddings can be thought of as lines in space. Two vector embeddings are 'similar' if the lines are close to each other. \n",
    "\n",
    "This is how our RAG system can identify the most relevant document chunks to feed to the LLM. The user query vector embedding is compared to the sub-document embeddings.\n",
    "\n",
    "L2 distance is not the only way to compare 2 vector embeddings. Another common choice is Cosine Similarity which measures the angle between 2 vectors in multi-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0209b9-9c65-4c1d-b4bb-cf47c220e09f",
   "metadata": {},
   "source": [
    "## 6. Build the RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad5c4f-87b0-42a3-ba08-3065216341a2",
   "metadata": {},
   "source": [
    "Now our documents have been chunked and stored in a vector database with their embeddings we are ready to define the components needed for our RAG system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48761b31-8587-45b3-aefa-2fc79bb887b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for this section\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a589b6f-36d0-4552-820e-929eec1f2485",
   "metadata": {},
   "source": [
    "#### Initialize the model\n",
    "Initialize the model using the models.corp credentials defined earlier (see section 2 - Get models.corp credentials).\n",
    "Note the low temperature parameter which will give us accurate, factual responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e365ede6-b2d9-43d4-95a7-2b723c5b46bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "llm = ChatOpenAI(model=model, api_key=access_token, base_url=model_api_url, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d9b3b-e67c-40da-91c1-c1d0b84e9b3f",
   "metadata": {},
   "source": [
    "#### Create a retriever object\n",
    "The vectorstore.as_retriever() method creates a retriever object, which acts as a specialized search interface for the vector database.\n",
    "\n",
    "This retriever takes a query, uses the embedding model to get the query embedding vector, uses the vector store's index to efficiently find the most semantically relevant documents by vector comparison, and returns the top k documents, ready to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2baf7479-b4ca-4d10-ae83-da0acbf42512",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bc748-b6fd-431d-8d44-0b631cf1e13a",
   "metadata": {},
   "source": [
    "#### Define the prompt template\n",
    "\n",
    "Note that we are using PromptTemplate and manually specifying the roles Human and Assistant (see section 3c)\n",
    "You can also see two placeholders. `{question}` will be filled by the user's query and `{context}` will be filled by the article chunks returned by the retriever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae93160a-8052-481b-be83-2a21331b5aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Human: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. Specify if the answer is in the context or not.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbfd32-cc70-4f13-9342-f132bd0f0d2f",
   "metadata": {},
   "source": [
    "#### Building the RAG chain\n",
    "The chain's first step is a dictionary with two keys, context and question. \n",
    "* \"context\" key: The input query is passed to the retriever, which fetches relevant documents. These documents are then piped to format_docs to be converted into a single string.\n",
    "* \"question\" key: The RunnablePassthrough() also receives the exact same input query. Its only job is to do nothing to it and pass it straight through.\n",
    "\n",
    "This output is then fed to the prompt, where the `context` and `question` placeholders are substituted by the retrieved document string and the user input query.\n",
    "\n",
    "The formatted prompt is passed to the LLM, and lastly the response is passed through an output parser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "377b4696-235e-4514-b102-f1ac94ac56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joins the returned documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "# builds the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f0bbe-c078-4a3f-b7fa-93d64cf1ccd6",
   "metadata": {},
   "source": [
    "## 7. Query and test the RAG system\n",
    "We can now invoke the RAG chain and test if it replies correctly to our queries with responses based on our stored documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82675e0c-ebc3-4904-8bff-34b36c7ccbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "Question :  What are the advantages of few shot prompting?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Response :  Few-shot prompting offers several advantages in the field of artificial intelligence, particularly with models like GPT. It allows AI models to understand and generate context-specific responses with minimal input data. This technique involves providing the AI with a few examples to guide its responses, enabling the model to infer the desired task from these examples without explicit programming.\n",
      "\n",
      "One key advantage is improved performance on a wide range of tasks. According to a study by Brown et al. (2020), few-shot learning can achieve performance comparable to full fine-tuning on a diverse set of tasks, even when the number of training examples is limited.\n",
      "\n",
      "Another advantage is the ability to handle tasks that the model has not been explicitly trained on. This is demonstrated in the provided context, where the model correctly identifies the sentiment of the text \"I think the vacation is okay\" as neutral, despite not being specifically trained for sentiment analysis.\n",
      "\n",
      "However, it's important to note that few-shot prompting is not perfect, especially for complex thinking tasks. As mentioned in the context, standard few-shot prompting can struggle with very detailed or niche requests, where specialized knowledge is needed.\n",
      "\n",
      "Reference(s):\n",
      "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\n",
      "\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the advantages of few shot prompting?\"\n",
    "\n",
    "res = rag_chain.invoke(query)\n",
    "print(\"--------------------------\\n\")\n",
    "print(\"Question : \",query)\n",
    "print(\"\\n--------------------------\\n\")\n",
    "print(\"Response : \",res)\n",
    "print(\"\\n--------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa060d-e216-40e1-9943-4e95d20abaa7",
   "metadata": {},
   "source": [
    "#### Explore the retrieved documents for comparison with the response\n",
    "We can invoke the retrieever directly to explore the documents that have been used to inform the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a98c991-7f75-4ae1-9394-97584110b45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retrieved Documents ---\n",
      "\n",
      "--- Document 1 ---\n",
      "\n",
      "Source: https://medium.com/@tuhinsharma121/mastering-prompt-engineering-a-beginners-guide-to-ai-interaction-2a28434ccb67\n",
      "\n",
      "Content: is:Output:When we won the game, we all started to farduddle in celebration.LimitationStandard few-shot prompting is good for many tasks, but it’s not perfect, especially for complex thinking tasks. Let’s show why that is.Prompt:The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.A: The answer is False.The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.A: The answer is True.The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.A: The answer is True.The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.A: The answer is False.The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. A:Output:The answer is True.3. Chain-of-Thought (CoT) PromptingChain-of-thought (CoT) prompting helps machines think through problems step-by-step to understand them better. You can use it with few-shot prompting, where the machine sees a few examples first, to improve how it handles more difficult tasks that need thinking before answering.ExamplePrompt:The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. A:Output:Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.3.a. Zero-shot CoT PromptingLet’s try a simple problem and see how the model performs:Prompt:I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?Output:11 applesThe answer is incorrect! Now Let’s try with the special prompt “Let’s think step by step.”.ExamplePrompt:I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?Let's think step by\n",
      "\n",
      "\n",
      "--- Document 2 ---\n",
      "\n",
      "Source: https://medium.com/@tuhinsharma121/mastering-prompt-engineering-a-beginners-guide-to-ai-interaction-2a28434ccb67\n",
      "\n",
      "Content: of AI models, as it showcases their ability to handle a wide variety of tasks and questions they have not explicitly been prepared for.ExamplePrompt:Classify the text into neutral, negative or positive. Text: I think the vacation is okay.Sentiment:Output:NeutralLimitationFirstly, these models often aren’t as accurate as models trained on specific tasks because they’re trying to generalize without direct examples. This can lead to errors or lower confidence in the results. Also, because they handle such a broad range of tasks, zero-shot models might struggle with very detailed or niche requests, where specialized knowledge is needed.Prompt:An example of a sentence that uses the word farduddle is:Output:The word \"farduddle\" doesn't appear to be a standard or recognized English word. Could it be a typo or a specific term from a particular context or language that you're exploring? If you have more details or a different spelling, please share, and I'll be happy to help with a sentence!2. Few Shot PromptingFew-shot prompting is a technique used in the field of artificial intelligence, particularly with models like GPT (Generative Pre-trained Transformer), to improve the model’s ability to understand and generate context-specific responses with minimal input data. Unlike zero-shot or one-shot methods, few-shot prompting involves providing the AI with a few examples to guide its responses, thereby helping the model infer the desired task from these examples without explicit programming.ExamplePrompt:A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:We were traveling in Africa and we saw these very cute whatpus. To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:Output:When we won the game, we all started to farduddle in celebration.LimitationStandard few-shot prompting is good for many tasks, but it’s not perfect, especially for complex thinking\n",
      "\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. Invoke the retriever directly to get the relevant documents\n",
    "\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "# 3. Print the retrieved documents to inspect their content\n",
    "print(\"--- Retrieved Documents ---\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Document {i+1} ---\\n\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\\n\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db48ffa-4a68-4c54-98ea-70eda2e85a3b",
   "metadata": {},
   "source": [
    "### Exploring and testing the RAG system\n",
    "Let's see what happens if we ask something that is not in the Meduim article documents. Is this the behaviour you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e06f1253-9b28-4175-be0f-41f38da0e962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "Question :  What is the capital of France?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Response :  The capital of France is Paris. It is the most populous city in France, with a population of approximately 2.14 million people within the city proper and over 12 million in the metropolitan area. Paris is known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral.\n",
      "\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "query_test = \"What is the capital of France?\"\n",
    "\n",
    "res = rag_chain.invoke(query_test)\n",
    "print(\"--------------------------\\n\")\n",
    "print(\"Question : \",query_test)\n",
    "print(\"\\n--------------------------\\n\")\n",
    "print(\"Response : \",res)\n",
    "print(\"\\n--------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecea8fa7-ec06-44a5-9466-b29c3da045ce",
   "metadata": {},
   "source": [
    "#### Again explore the retrieved documents for comparison with the response\n",
    "Can you see any information related to the query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba22654f-d21e-4490-adfb-e48e121a8d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retrieved Documents ---\n",
      "\n",
      "--- Document 1 ---\n",
      "\n",
      "Source: https://medium.com/@fassha08/transforming-search-ai-agents-and-multi-vector-intelligence-1bde1dbe66e7\n",
      "\n",
      "Content: langchain_core.output_parsers import StrOutputParserfrom semantic_router import Routefrom semantic_router.encoders import HuggingFaceEncoderfrom semantic_router.layer import RouteLayerfrom langchain_community.tools.tavily_search import TavilySearchResultsimport osInstantiate the Embedding Modelembeddings = HuggingFaceEmbeddings(            model_name=\"BAAI/bge-large-en-v1.5\")Vector DatabasesIn this blog, we are using LangChain wrappers to connect to different vector databases. For more information on setting up PgVector, please refer to the PgVector documentation. Similarly, for Qdrant, you can refer to the Qdrant documentation. Regardless of which vector store we are using, the process remains the same.Data UsedDataset1: Technical documentationDomain: Software DevelopmentVector Database: PGVectorDataset2: Customer Service FAQsDomain: E-commerceVector Database: QdrntSetup PgVector DbDocuments can be loaded from the local hard disk or from web URLs, based on the requirement.#from localloader = DirectoryLoader('./data/pg/', glob=\"./*.txt\") documents = loader.load()#from web just for example purpose, not used in the blog urls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]docs = [WebBaseLoader(url).load() for url in urls]documents = [item for sublist in docs for item in sublist]print(f\"len of documents :{len(documents)}\")Create chunkstext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)docs = text_splitter.split_documents(documents)Lets see how are software domain data chunks looks likeAdd the documents to Pg vector#Cloud hosted Pg vector databaseconnection = \"postgresql://url here\"  collection_name = \"pg_data\"pg_vector_store = PGVector(        connection=connection,         collection_name=collection_name,         embeddings=embeddings    )pg_vector_store.add_documents(docs)Instantiate\n",
      "\n",
      "\n",
      "--- Document 2 ---\n",
      "\n",
      "Source: https://medium.com/@harshkumar1146/prompt-chaining-unlocking-the-full-potential-of-ai-assistants-4fdf2f28c1a5\n",
      "\n",
      "Content: key attractions, accommodations, and transportation.Step 1: Destination OverviewPrompt 1:I want to plan a 7-day trip to Germany. Can you suggest an itinerary?Output:Sure! Here's a suggested 7-day itinerary for Germany:Day 1-2: Berlin - Explore historical sites like the Brandenburg Gate,          Berlin Wall, and Museum Island.Day 3:   Dresden - Visit the Frauenkirche and Zwinger Palace.Day 4-5: Munich - Enjoy Marienplatz, Nymphenburg Palace, and the famous          beer gardens.Day 6:   Neuschwanstein Castle - Take a day trip to this fairy-tale castle.Day 7:   Frankfurt - Tour the Römer, Städel Museum, and Main Tower.Step 2: Break Down the Trip by DayPrompt:Great! Now, let's break down the itinerary day by day. What should I do onthe first day in Berlin?Output:On the first day in Berlin, you can:1. Morning: Visit the Brandenburg Gate and Reichstag Building.2. Afternoon: Explore the Berlin Wall Memorial and Checkpoint Charlie.3. Evening: Stroll along Unter den Linden and have dinner at a local   restaurant.Step 3: Add Details and PreferencesPrompt:I love German cuisine. Can you recommend some must-try restaurants in Berlin for day one? [Output from step 2]Output:Sure! Here are some must-try restaurants in Berlin:1. Frühstück 3000 for a unique breakfast experience.2. Zur Letzten Instanz, Berlin's oldest restaurant, for traditional    German dishes.3. Curry 36 for the famous currywurst.Step 4: Incorporate LogisticsPrompt:How should I travel from Berlin to Dresden on the third day?[Output from step 1]Output:You can travel from Berlin to Dresden by train. It's a comfortable 2-hourjourney with frequent departures. Consider booking tickets in advance through Deutsche Bahn for the best rates.Step 5: Customize Based on FeedbackPrompt:I prefer outdoor activities. Are there any hiking spots near Munich that Ican visit on day five? [Output from step 1]Output:Absolutely! Near Munich, you can hike in the Bavarian Alps. A popular trailis the Partnach Gorge in\n",
      "\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. Invoke the retriever directly to get the relevant documents\n",
    "\n",
    "retrieved_docs = retriever.invoke(query_test)\n",
    "\n",
    "# 3. Print the retrieved documents to inspect their content\n",
    "print(\"--- Retrieved Documents ---\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Document {i+1} ---\\n\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\\n\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13518b62-f001-4c32-a100-6816d6736227",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "You have successfully built a simple **Retrieval Augmented Generation (RAG) question** and answer system based on publicly available Medium articles.\n",
    "\n",
    "Well done!\n",
    "\n",
    "#### To go further\n",
    "- Experiment with different parameters and prompts\n",
    "- Try other models from models.corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff47182-c364-4e92-a350-0002b1acb6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
