{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29946e6d-4745-441c-9614-4b584b59eef9",
   "metadata": {},
   "source": [
    "# Lab: Using LangChain to build a simple RAG pipeline\n",
    "\n",
    "## Goal\n",
    "\n",
    "Explore basic LangChain and other component functionality with the ultimate goal of developing a simple **Retrieval Augmented Generation (RAG)** question and answer system based on publicly available Medium articles.\n",
    "\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. Install dependencies\n",
    "2. Quick LangChain tutorial (optional)\n",
    "3. Download and transform data\n",
    "4. Generate embeddings and add to vector store\n",
    "5. Build the RAG chain\n",
    "6. Query and test the RAG system\n",
    "    \n",
    "---\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "#### Python\n",
    "This notebook has been tested with both Python `3.12` and `3.13` \n",
    "\n",
    "### Expected Outcome:\n",
    "\n",
    "A robust pipeline capable of:\n",
    "- Extracting content from urls.\n",
    "- Generating high-quality embeddings.\n",
    "- Querying with contextually relevant answers.\n",
    "- Providing accurate responses to complex user queries in a highly scalable manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5873d-3a82-455d-82e6-daab008dc4e7",
   "metadata": {},
   "source": [
    "## 1. Install dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b7641-091f-4573-b911-43a7398f5434",
   "metadata": {},
   "source": [
    "## Introduction to the Jupyter Lab Environment\n",
    "\n",
    "\n",
    "JupyterLab Basics\n",
    "JupyterLab is an interactive development environment where you can run code in \"cells\" and see the results immediately. Here's how to navigate:\n",
    "\n",
    "To execute a cell in Jupyter Lab, use one of the following methods:\n",
    "\n",
    "* Press <kbd>Shift</kbd> + <kbd>Enter</kbd> to run the current cell and move on to the next cell\n",
    "\n",
    "* Press <kbd>Ctrl</kbd> + <kbd>Enter</kbd> to run the cell and stay in the same cell.\n",
    "\n",
    "Run Button: Click the ▶️ \"Run\" button in the toolbar above, it looks like a play button.\n",
    "\n",
    "(In Jupyter Lab Notebooks, you can enable Code cells to run shell commands by prefixing them with an exclamation mark (!).)\n",
    "\n",
    "Let's use this to install the LangChain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "921dbf04-1026-4d69-9c7b-6d94ace8a324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /home/dev/venv/lib64/python3.12/site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain_openai in /home/dev/venv/lib64/python3.12/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain_milvus in /home/dev/venv/lib64/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: langchain_text_splitters in /home/dev/venv/lib64/python3.12/site-packages (0.3.8)\n",
      "Requirement already satisfied: langchain_huggingface in /home/dev/venv/lib64/python3.12/site-packages (0.3.0)\n",
      "Requirement already satisfied: langchain_core in /home/dev/venv/lib64/python3.12/site-packages (0.3.66)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_community) (0.3.26)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_community) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_community) (3.12.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_community) (2.3.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_openai) (1.91.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: pymilvus<3.0,>=2.5.7 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_milvus) (2.5.11)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_huggingface) (0.21.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.30.2 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_huggingface) (0.33.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_core) (4.14.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /home/dev/venv/lib64/python3.12/site-packages (from langchain_core) (2.11.7)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/dev/venv/lib64/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/dev/venv/lib64/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dev/venv/lib64/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/dev/venv/lib64/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/dev/venv/lib64/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/dev/venv/lib64/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/dev/venv/lib64/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/dev/venv/lib64/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/dev/venv/lib64/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: filelock in /home/dev/venv/lib64/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dev/venv/lib64/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2025.5.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/dev/venv/lib64/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/dev/venv/lib64/python3.12/site-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (1.1.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/dev/venv/lib64/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/dev/venv/lib64/python3.12/site-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/dev/venv/lib64/python3.12/site-packages (from langsmith>=0.1.125->langchain_community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/dev/venv/lib64/python3.12/site-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/dev/venv/lib64/python3.12/site-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/dev/venv/lib64/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/dev/venv/lib64/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/dev/venv/lib64/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /home/dev/venv/lib64/python3.12/site-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/dev/venv/lib64/python3.12/site-packages (from pydantic>=2.7.4->langchain_core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/dev/venv/lib64/python3.12/site-packages (from pydantic>=2.7.4->langchain_core) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/dev/venv/lib64/python3.12/site-packages (from pydantic>=2.7.4->langchain_core) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/dev/venv/lib64/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
      "Requirement already satisfied: setuptools>69 in /home/dev/venv/lib64/python3.12/site-packages (from pymilvus<3.0,>=2.5.7->langchain_milvus) (80.9.0)\n",
      "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /home/dev/venv/lib64/python3.12/site-packages (from pymilvus<3.0,>=2.5.7->langchain_milvus) (1.67.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /home/dev/venv/lib64/python3.12/site-packages (from pymilvus<3.0,>=2.5.7->langchain_milvus) (5.29.5)\n",
      "Requirement already satisfied: ujson>=2.0.0 in /home/dev/venv/lib64/python3.12/site-packages (from pymilvus<3.0,>=2.5.7->langchain_milvus) (5.10.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /home/dev/venv/lib64/python3.12/site-packages (from pymilvus<3.0,>=2.5.7->langchain_milvus) (2.3.0)\n",
      "Requirement already satisfied: milvus-lite>=2.4.0 in /home/dev/venv/lib64/python3.12/site-packages (from pymilvus<3.0,>=2.5.7->langchain_milvus) (2.4.12)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/dev/venv/lib64/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dev/venv/lib64/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dev/venv/lib64/python3.12/site-packages (from requests<3,>=2->langchain_community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dev/venv/lib64/python3.12/site-packages (from requests<3,>=2->langchain_community) (2025.6.15)\n",
      "Requirement already satisfied: greenlet>=1 in /home/dev/venv/lib64/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/dev/venv/lib64/python3.12/site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in /home/dev/venv/lib64/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/dev/venv/lib64/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/dev/venv/lib64/python3.12/site-packages (from pandas>=1.2.4->pymilvus<3.0,>=2.5.7->langchain_milvus) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dev/venv/lib64/python3.12/site-packages (from pandas>=1.2.4->pymilvus<3.0,>=2.5.7->langchain_milvus) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/dev/venv/lib64/python3.12/site-packages (from pandas>=1.2.4->pymilvus<3.0,>=2.5.7->langchain_milvus) (2025.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/dev/venv/lib64/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/dev/venv/lib64/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus<3.0,>=2.5.7->langchain_milvus) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community langchain_openai langchain_milvus langchain_text_splitters langchain_huggingface langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ccb1c4-b995-4227-93b2-a3be8c3af100",
   "metadata": {},
   "source": [
    "Now we can install other important Python dependencies needed by our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d191cc-1e6a-48f5-9c5c-bf80f2216e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /home/dev/venv/lib64/python3.12/site-packages (4.13.4)\n",
      "Requirement already satisfied: python-dotenv in /home/dev/venv/lib64/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: chromadb in /home/dev/venv/lib64/python3.12/site-packages (1.0.13)\n",
      "Requirement already satisfied: sentence-transformers in /home/dev/venv/lib64/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/dev/venv/lib64/python3.12/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/dev/venv/lib64/python3.12/site-packages (from beautifulsoup4) (4.14.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (2.11.7)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (2.3.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (6.0.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (0.21.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (1.67.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /home/dev/venv/lib64/python3.12/site-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/dev/venv/lib64/python3.12/site-packages (from sentence-transformers) (4.53.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/dev/venv/lib64/python3.12/site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in /home/dev/venv/lib64/python3.12/site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in /home/dev/venv/lib64/python3.12/site-packages (from sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/dev/venv/lib64/python3.12/site-packages (from sentence-transformers) (0.33.0)\n",
      "Requirement already satisfied: Pillow in /home/dev/venv/lib64/python3.12/site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: packaging>=19.1 in /home/dev/venv/lib64/python3.12/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /home/dev/venv/lib64/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: anyio in /home/dev/venv/lib64/python3.12/site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/dev/venv/lib64/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /home/dev/venv/lib64/python3.12/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/dev/venv/lib64/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/dev/venv/lib64/python3.12/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: filelock in /home/dev/venv/lib64/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dev/venv/lib64/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: requests in /home/dev/venv/lib64/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/dev/venv/lib64/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/dev/venv/lib64/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/dev/venv/lib64/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/dev/venv/lib64/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/dev/venv/lib64/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/dev/venv/lib64/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/dev/venv/lib64/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/dev/venv/lib64/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/dev/venv/lib64/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /home/dev/venv/lib64/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/dev/venv/lib64/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /home/dev/venv/lib64/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /home/dev/venv/lib64/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: coloredlogs in /home/dev/venv/lib64/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/dev/venv/lib64/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /home/dev/venv/lib64/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /home/dev/venv/lib64/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /home/dev/venv/lib64/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/dev/venv/lib64/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /home/dev/venv/lib64/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in /home/dev/venv/lib64/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /home/dev/venv/lib64/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/dev/venv/lib64/python3.12/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /home/dev/venv/lib64/python3.12/site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/dev/venv/lib64/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/dev/venv/lib64/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/dev/venv/lib64/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/dev/venv/lib64/python3.12/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/dev/venv/lib64/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: setuptools in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: networkx in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/dev/venv/lib64/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dev/venv/lib64/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/dev/venv/lib64/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/dev/venv/lib64/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/dev/venv/lib64/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /home/dev/venv/lib64/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /home/dev/venv/lib64/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/dev/venv/lib64/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/dev/venv/lib64/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/dev/venv/lib64/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/dev/venv/lib64/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/dev/venv/lib64/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/dev/venv/lib64/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/dev/venv/lib64/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/dev/venv/lib64/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/dev/venv/lib64/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/dev/venv/lib64/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/dev/venv/lib64/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/dev/venv/lib64/python3.12/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/dev/venv/lib64/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dev/venv/lib64/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /home/dev/venv/lib64/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 python-dotenv chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7899d-d416-495b-ae26-7067186bc9d0",
   "metadata": {},
   "source": [
    "## 2. Start your LLM Locally\n",
    "\n",
    "In this lab we will use the local Nvidia GPU and LLM runtime, `ollama` to run the 8 Billion parameter `granite3.3:latest` LLM locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a03c348-e58a-4302-819e-47effa8e0e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['ollama', 'run', 'granite3.3', '--keepalive'...>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?25l\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.Popen([\"ollama\", \"run\", \"granite3.3\", \"--keepalive\", \"-1m\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d36c4-01e4-4e0f-9aab-42a88a84933b",
   "metadata": {},
   "source": [
    "We can easily check if the LLM, Granite, is now running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a2bc239-a500-4fcc-af23-426cdaff54b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                 ID              SIZE      PROCESSOR    UNTIL   \n",
      "granite3.3:latest    fd429f23b909    7.8 GB    100% GPU     Forever    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l"
     ]
    }
   ],
   "source": [
    "!ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb56e96-629f-4a5a-9c9f-0f322791212e",
   "metadata": {},
   "source": [
    "We can also validate that our model is actually being servered, by Ollama, in a way that our application will be able to access.\n",
    "A simple test to see if our model is being served is to query the LLM runtime with curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "421809bd-2153-4313-8e69-14bcb3dc50ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"object\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"list\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"data\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "    \u001b[1;39m{\n",
      "      \u001b[0m\u001b[34;1m\"id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"granite3.3:latest\"\u001b[0m\u001b[1;39m,\n",
      "      \u001b[0m\u001b[34;1m\"object\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"model\"\u001b[0m\u001b[1;39m,\n",
      "      \u001b[0m\u001b[34;1m\"created\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1750703969\u001b[0m\u001b[1;39m,\n",
      "      \u001b[0m\u001b[34;1m\"owned_by\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"library\"\u001b[0m\u001b[1;39m\n",
      "    \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m]\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751313075.406341    2639 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!TOKENIZERS_PARALLELISM=false curl -s http://localhost:11434/v1/models | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e52d2-69c8-4e59-9f86-c32952c65695",
   "metadata": {},
   "source": [
    "We can actually also use `curl` to talk directly to `granite` via the OpenAI API for **inference**:\n",
    "\n",
    "This command may take awhile to run, but less than a minute, as it is generating 5 - 10 paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db18af11-b652-4bd9-86d6-91a6c21131c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-251\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1751313133,\n",
      "  \"model\": \"granite3.3\",\n",
      "  \"system_fingerprint\": \"fp_ollama\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Red Hat Enterprise Linux (RHEL) is a commercial open-source operating system (OS) developed by Red Hat, an American software company. It's renowned for its stability, robustness, and extensive support, making it a popular choice for enterprise environments. Here are some key aspects of RHEL:\\n\\n1. **Development and Support**: RHEL is built from the source code of the community-driven Fedora project, ensuring it stays updated with the latest OS technologies. Red Hat Inc., however, provides long-term support (up to 10 years) for each major version, which is a significant advantage for businesses that require predictable and stable environments.\\n\\n2. **Community and Commercial Collaboration**: RHEL benefits from both community contributions and commercial backing. Developers from around the world contribute to its open-source nature while Red Hat Inc. adds enterprise-grade features, extensive documentation, and professional support services. This collaboration results in a powerful OS that meets diverse user needs.\\n\\n3. **Security**: Security is a core focus in RHEL's development. It incorporates SELinux (Security-Enhanced Linux), a mandatory access control mechanism that provides strong, flexible access controls. Regular security updates and patches are provided by Red Hat, ensuring the OS remains protected against emerging threats.\\n\\n4. **Stability and Scalability**: RHEL is designed for stability and high availability, meaning it can operate reliably over long periods without interruptions. It includes features such as live kernel patching, which allows system administrators to apply critical security patches without rebooting the system, thus minimizing downtime. This makes it suitable for large-scale deployments in mission-critical environments.\\n\\n5. **Virtualization and Containerization**: RHEL offers robust support for virtualization technologies like KVM (Kernel-based Virtual Machine) and containerization platforms such as Red Hat OpenShift and Docker. These features enable businesses to efficiently manage resources, deploy applications, and scale their infrastructure as needed.\\n\\n6. **Software Management**: The YUM (Yellowdog Updater, Modified) package manager in RHEL simplifies software installation, updates, and removals. It ensures that users can easily access and maintain a wide range of open-source software without complex compilation processes or manual dependency management.\\n\\n7. **Documentation and Support Ecosystem**: Red Hat provides comprehensive documentation for RHEL, including installation guides, user manuals, and troubleshooting resources. Additionally, their extensive support ecosystem offers technical assistance through various channels, ensuring that users can resolve issues promptly and efficiently.\\n\\n8. **Certifications and Compliance**: RHEL meets stringent compliance standards required by various industries (such as healthcare with HIPAA, finance with PCI DSS) and is certified for use in critical systems by organizations like the US Department of Defense. This makes it a reliable choice for businesses dealing with sensitive data or subject to regulatory requirements.\\n\\n9. **Interoperability**: RHEL works seamlessly with other Red Hat products, such as JBoss (middleware), OpenShift (container platform), and GlusterFS (distributed file system). This interoperability allows businesses to build a cohesive IT infrastructure that leverages the strengths of each component.\\n\\n10. **Cost-effectiveness**: While RHEL is a commercial product, it can prove cost-effective for enterprises in the long run due to its stability, longevity, and comprehensive support. By avoiding frequent OS upgrades and minimizing downtime through features like live kernel patching, businesses can save on resources and focus on their core operations.\\n\\nIn conclusion, Red Hat Enterprise Linux is a powerful, secure, and stable operating system tailored for enterprise use. Its strong community support, extensive documentation, compliance certifications, and interoperability with other Red Hat products make it an attractive choice for businesses seeking a reliable foundation for their IT infrastructure.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 55,\n",
      "    \"completion_tokens\": 845,\n",
      "    \"total_tokens\": 900\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -s -X POST http://localhost:11434/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"granite3.3\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write me 5 to 10 paragraphs about RHEL\"\n",
    "      }\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 5000\n",
    "  }' | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a70465-3851-459a-b99c-7d986cab4d59",
   "metadata": {},
   "source": [
    "## Getting Setup\n",
    "\n",
    "Now we have a running LLM we can query, lets setup our Python variables which we will need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df568aa4-cbad-4502-ad49-7d6a8aa27b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USER_AGENT'] = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    \"Chrome/120.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "api_key = \"dummy\"\n",
    "model = \"granite3.3\"\n",
    "base_url=\"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb0e9d-ecfa-4408-8c7c-1636540e36b0",
   "metadata": {},
   "source": [
    "## 3. Quick LangChain tutorial (optional)\n",
    "\n",
    "*NOTE: This section does not contribute to the RAG system. It is included for testing and exploration.* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec59ba34-6eb6-4f4a-9ea8-27da68118113",
   "metadata": {},
   "source": [
    "LangChain is a powerful framework that simplifies the development of AI applications by providing a standard way to chain together language models with external data sources and other components and tools.\n",
    "\n",
    "In LangChain, components are the modular, reusable building blocks of an AI application, such as Models, Prompt Templates, and Output Parsers. These individual components are then linked together into chains, which define a complete workflow for a specific task, such as answering a question or summarizing a document. Using the LangChain Expression Language (LCEL), you can connect these components, creating an automated data flow from the initial user input to the final, processed output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d87f9d-fafd-4b86-b4ee-5516044c510e",
   "metadata": {},
   "source": [
    "### 3a. Models: the core engine\n",
    "A Model in LangChain is a wrapper around a large language model like Granite (the model we are using for this lab).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f15a20a-e610-47e4-972d-25812f4ade38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The official currency of France is the Euro (€). The Euro was introduced as a common currency for participating countries in the European Union on January 1, 1999, and it replaced the French Franc. This monetary union, known as the Eurozone, now consists of 19 member states, with France being one of them.\n",
      "\n",
      "The Euro is managed and controlled by the European Central Bank (ECB), which is headquartered in Frankfurt, Germany. The ECB sets monetary policy, manages foreign exchange reserves, and oversees the issuance of Euro banknotes and coins.\n",
      "\n",
      "If you have any more questions about the Euro, French currency, or related topics, please feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# minimum import needed\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Temperature loose guidelines: 0-0.4 LOW deterministic;  0.5-1.0  MEDIUM balanced creativey and coherence; 1.1-2 HIGH creative and random\n",
    "\n",
    "# Initialize the model. Set a low temperature for predictable, less creative responses\n",
    "testllm = ChatOpenAI(model=model, api_key=api_key, base_url=base_url, temperature=0.1)\n",
    "\n",
    "# We can now \"invoke\" the model with a simple prompt\n",
    "response = testllm.invoke(\"What is the official currency of France?\")\n",
    "\n",
    "# The response is an AI Message object, so we access its content\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354694d2-40be-412e-86e6-fccd49438f71",
   "metadata": {},
   "source": [
    "### 3b. Output Parsers: Structuring the Response\n",
    "\n",
    "The output from an LLM is typically an AI Message object. An output parser is a class that helps you structure the model's response into a more usable format, like a simple string, a list, or a JSON object.\n",
    "\n",
    "In the last exercise, we used `response.content` to get the content string from our model response. Here we will use an output parser component which we can use later in a LangChain chain. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "acb518f2-c5e0-4283-bfd9-41841294ab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Josephine Baker (1891-1975) was an American-born French entertainer and civil rights activist, celebrated for her dazzling performances in Parisian nightclubs. Known as the \"Black Venus,\" she broke racial barriers with her exotic dance routines, captivating audiences worldwide. Beyond showbiz, Baker was a dedicated anti-colonial and civil rights campaigner, supporting the Civil Rights Movement in the US and fighting against apartheid in South Africa. She was posthumously awarded France's Legion of Honor and remains an enduring symbol of resilience and artistic brilliance.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# The llm.invoke() call returns an AIMessage object\n",
    "response = testllm.invoke(\"Give me a 60 word biography on Josephine Baker\")\n",
    "\n",
    "# The parser converts the AIMessage into a simple string\n",
    "parsed_output = output_parser.invoke(response)\n",
    "\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88814be-8041-4eac-80b7-b35f47b8df41",
   "metadata": {},
   "source": [
    "### 3c. Prompt Templates: Crafting Your Instructions\n",
    "A prompt template in LangChain is a reusable object that creates a complete and formatted prompt for a language model by dynamically inserting user inputs and other variables into a predefined text structure.\n",
    "\n",
    "We will compare two prompt template classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0001d-5cf0-4d30-8304-d97e8d2b6fc9",
   "metadata": {},
   "source": [
    "#### PromptTemplate\n",
    "This class creates a single string from a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "caaa5513-ddd9-4a83-9343-ec8dcabe28b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Translate the expression little by little from English to French.'\n",
      "Type: <class 'langchain_core.prompt_values.StringPromptValue'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define a PromptTemplate object with a placeholder for a word to translate\n",
    "PT_prompt = PromptTemplate(\n",
    "    template=\"Translate the expression {toTranslate} from English to French.\",\n",
    "    input_variables=[\"toTranslate\"]\n",
    ")\n",
    "\n",
    "#invoke the template and print the output and type \n",
    "formatted_PT_prompt=PT_prompt.invoke({\"toTranslate\": \"little by little\"})\n",
    "\n",
    "print(formatted_PT_prompt)\n",
    "print(f\"Type: {type(formatted_PT_prompt)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8f266-8799-4ad4-853d-62b8a41d3d3e",
   "metadata": {},
   "source": [
    "#### ChatPromptTemplate\n",
    "This class creates a structured list of messages for chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3c10c3a-994f-430f-aac1-e69b77d38fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Translate the expression little by little from English to French.', additional_kwargs={}, response_metadata={})]\n",
      "Type: <class 'langchain_core.prompt_values.ChatPromptValue'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a ChatPromptTemplate object with a placeholder for a word to translate\n",
    "CPT_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the expression {toTranslate} from English to French.\"\n",
    ")\n",
    "\n",
    "#invoke the template and print the output and type \n",
    "formatted_CPT_prompt=CPT_prompt.invoke({\"toTranslate\": \"little by little\"})\n",
    "\n",
    "print(formatted_CPT_prompt)\n",
    "print(f\"Type: {type(formatted_CPT_prompt)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c17a9b3-81b0-4bb7-b973-64db34d35668",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "`ChatPromptTemplate` creates a structured list of messages whereas `PromptTemplate`creates a single string.\n",
    "\n",
    "`ChatPromptTemplate` is used more commonly because modern models are optimised for a sequence of messages. They perform best when they receive a structured list of messages with roles (System for instructions, Human for user input), and ChatPromptTemplate is designed specifically for this.\n",
    "However it is also possible to format those roles manually with`PromptTemplate` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9ac0d8-8d13-40bf-a8f3-a132108b3f4d",
   "metadata": {},
   "source": [
    "### 3d. LangChain Expression Language (LCEL): Chaining It All Together\n",
    "\n",
    "LCEL is the declarative syntax used to chain LangChain components together. It uses the pipe symbol ( | ). Data flows from one component to the next in the sequence.\n",
    "\n",
    "We will create a simple chain using use the `testllm` model, the `PT_Prompt` template, and the `output_parser` from the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e14bb11-d6f2-48a1-96b6-807bdb265e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Félicitations pour la construction de cette chaîne.\n"
     ]
    }
   ],
   "source": [
    "# Build the chain\n",
    "test_chain=PT_prompt | testllm | output_parser\n",
    "\n",
    "# Invoke the chain with the input data \"congratulations on building this chain\" (the expression we want to translate into French)\n",
    "chain_output = test_chain.invoke(\"congratulations on building this chain\")\n",
    "\n",
    "print(chain_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfecd118-8298-4f8c-b4f5-3be1fc735f61",
   "metadata": {},
   "source": [
    "Congratulations on your first LangChain chain!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae264e-6b4f-4f77-8657-d817d4c9a14f",
   "metadata": {},
   "source": [
    "## 4. Download and transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4465c0d-1445-4c13-938c-902015484169",
   "metadata": {},
   "source": [
    "The example uses multiple Medium articles on the subject of generative AI as the source document.\n",
    "\n",
    "We will load the documents and split them into shorter document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a30d7dd4-a4ba-43ec-98dc-053d70300afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Instantiate the WebBaseLoader to fetch content from specified URLs.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\n",
    "        \"https://medium.com/@tuhinsharma121/mastering-prompt-engineering-a-beginners-guide-to-ai-interaction-2a28434ccb67\",\n",
    "        \"https://medium.com/@rahuljangir2992/graph-based-prompting-revolutionizing-ai-reasoning-f316b7266c1f\",\n",
    "        \"https://medium.com/@fassha08/transforming-search-ai-agents-and-multi-vector-intelligence-1bde1dbe66e7\",\n",
    "        \"https://medium.com/@harshkumar1146/prompt-chaining-unlocking-the-full-potential-of-ai-assistants-4fdf2f28c1a5\",\n",
    "    ),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The load() method fetches and parses the content from the URLs returning a list of Document objects, where each object contains the text content of one webpage.\n",
    "documents = loader.load()\n",
    "\n",
    "# Initialize the RecursiveCharacterTextSplitter which will break down large texts into smaller chunks.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9689c029-bf6f-4d27-99ae-085c557e0392",
   "metadata": {},
   "source": [
    "#### Exploring the document chunks\n",
    "\n",
    "*Note: This does not contribute to building the RAG system*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58002b80-3ab7-446e-a09f-5b448f2b0bd3",
   "metadata": {},
   "source": [
    "The output, `docs`, is a flat list of all the text chunks derived from the original documents. These smaller, more granular pieces of text are now in an ideal format to be used for creating vector embeddings for a similarity search in a RAG pipeline.\n",
    "\n",
    "If we look at the first two list items we can see the included metadata and note the overlap in the `page_content` chunks. \n",
    "Without overlap, you risk cutting a sentence or a complete thought exactly in half, leading to two incoherent chunks that lose their meaning. This severely damages the ability of the RAG system to both find the right information (retrieval) and understand it correctly (generation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8730bfc9-45f7-4e2c-bfd5-73ca1079dd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The articles have been split into 38 sub-documents. \n",
      "\n",
      "First sub-document:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://medium.com/@tuhinsharma121/mastering-prompt-engineering-a-beginners-guide-to-ai-interaction-2a28434ccb67', 'title': 'Mastering Prompt Engineering: A Beginner’s Guide to AI Interaction | by Tuhin Sharma | Medium', 'description': 'In today’s world of artificial intelligence (AI), prompt engineering has become a key skill. It changes how we talk to AI models and make them work better. Whether you’re experienced or just…', 'language': 'en'}, page_content='Mastering Prompt Engineering: A Beginner’s Guide to AI Interaction | by Tuhin Sharma | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inMastering Prompt Engineering: A Beginner’s Guide to AI InteractionTuhin Sharma11 min read·Jul 10, 2024--1ListenShareIn today’s world of artificial intelligence (AI), prompt engineering has become a key skill. It changes how we talk to AI models and make them work better. Whether you’re experienced or just starting, knowing how to create good prompts can make a big difference in how well your AI applications perform.Prompt engineering is about designing the questions or instructions we give to AI models. This helps them give accurate and useful answers. It’s not just a technical skill; it’s also about using creativity and careful thinking to get the best results from AI.In this blog, we’ll explore what prompt engineering is, how it works, and why it matters. You’ll learn simple techniques and see real-life examples of how it can improve things like chatbots and content creation. Join us as we uncover the basics of prompt engineering and how it can help you make smarter, more effective AI solutions.1. Zero Shot PromptingZero-shot prompting is a technique used in the field of artificial intelligence where a model generates responses or performs tasks without any prior specific training on those tasks. This approach relies on the general knowledge and capabilities that the model has acquired during its initial comprehensive training phase. In zero-shot prompting, the model is given a task description or a prompt and is expected to understand and execute the task based solely on this input, without any examples or additional guidance. This method is particularly valuable for testing the flexibility and adaptability of AI models, as it showcases their ability to handle a wide variety of tasks and questions they have not explicitly been prepared for.ExamplePrompt:Classify the text into neutral, negative or positive.')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"The articles have been split into {len(docs)} sub-documents. \\n\")\n",
    "print (\"First sub-document:\")\n",
    "docs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f07c757-e7ec-48d7-988d-3d75b1217fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second sub-document:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://medium.com/@tuhinsharma121/mastering-prompt-engineering-a-beginners-guide-to-ai-interaction-2a28434ccb67', 'title': 'Mastering Prompt Engineering: A Beginner’s Guide to AI Interaction | by Tuhin Sharma | Medium', 'description': 'In today’s world of artificial intelligence (AI), prompt engineering has become a key skill. It changes how we talk to AI models and make them work better. Whether you’re experienced or just…', 'language': 'en'}, page_content='AI models, as it showcases their ability to handle a wide variety of tasks and questions they have not explicitly been prepared for.ExamplePrompt:Classify the text into neutral, negative or positive. Text: I think the vacation is okay.Sentiment:Output:NeutralLimitationFirstly, these models often aren’t as accurate as models trained on specific tasks because they’re trying to generalize without direct examples. This can lead to errors or lower confidence in the results. Also, because they handle such a broad range of tasks, zero-shot models might struggle with very detailed or niche requests, where specialized knowledge is needed.Prompt:An example of a sentence that uses the word farduddle is:Output:The word \"farduddle\" doesn\\'t appear to be a standard or recognized English word. Could it be a typo or a specific term from a particular context or language that you\\'re exploring? If you have more details or a different spelling, please share, and I\\'ll be happy to help with a sentence!2. Few Shot PromptingFew-shot prompting is a technique used in the field of artificial intelligence, particularly with models like GPT (Generative Pre-trained Transformer), to improve the model’s ability to understand and generate context-specific responses with minimal input data. Unlike zero-shot or one-shot methods, few-shot prompting involves providing the AI with a few examples to guide its responses, thereby helping the model infer the desired task from these examples without explicit programming.ExamplePrompt:A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:We were traveling in Africa and we saw these very cute whatpus. To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:Output:When we won the game, we all started to farduddle in celebration.LimitationStandard few-shot prompting is good for many tasks, but it’s not perfect, especially for complex thinking')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Second sub-document:\")\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab10660-1d91-418a-b759-a1839feea4cc",
   "metadata": {},
   "source": [
    "## 5. Generate embeddings and add to vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b85fe-a5b2-4bfb-bea2-7b33fd4d5569",
   "metadata": {},
   "source": [
    "### 5a. Embeddings\n",
    "Vector embeddings are dense numerical representations of data (our article chunks in this lab). An embedding is essentially an array of numbers that can represent a vector in multidimensional space. These vector representations position similar concepts closer together within this high-dimensional vector space, creating a semantic map of the text.\n",
    "\n",
    "To generate embeddings from our text chunks we use an embedding model, mxbai-embed-large-v1, from HuggingFace. This model converts each text chunk into an array with 1024 dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "927b7933-4dc7-4f97-a3f3-042d95891098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee40315903d44d8af35cc8b9b448b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f95596937344a3c960a62cbaa5014f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076768896c174e2f9919dc66eaebee7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61037620111d4d5d81ffb922dae5f920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f602ab67f3e47eb912272cffcdef24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b85c61991ea4f0a8f66c819077689df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8102ddada740b98481a4ecffc41250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e929d7e3beda418fbe87af6a007fc2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2545fdc31b0b4bac97f8f6e636f55c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e8ba1637974f7eafb66cfbfc7eef51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7d19dbe9a64ff7a49632dee1703ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Specify the embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd85927-7865-40d9-988e-c6f6ac3d7977",
   "metadata": {},
   "source": [
    "#### Exploring embeddings\n",
    "We can explore an embedding on some sample text.\n",
    "\n",
    "*Note: This does not contribute to building the RAG system*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f584e9dd-351e-48e7-b985-368c22f3258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dimensions 1024\n",
      "Data type of the numbers <class 'float'>\n",
      "Here are the first 5 dimensions (numbers) of the vector\n",
      "[0.15680921077728271, 0.267135351896286, -0.11924470216035843, 0.4239083230495453, -1.2445896863937378]\n"
     ]
    }
   ],
   "source": [
    "sample_text=\"What are the advantages of few shot prompting?\"\n",
    "\n",
    "vector_embedding = embeddings.embed_query(sample_text)\n",
    "\n",
    "# Print some information about the vector\n",
    "print(f\"Number of dimensions {len(vector_embedding)}\")\n",
    "print(f\"Data type of the numbers {type(vector_embedding[0])}\")\n",
    "\n",
    "# Print the first few numbers to give a sense of what it looks like\n",
    "print(\"Here are the first 5 dimensions (numbers) of the vector\")\n",
    "print(vector_embedding[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596be2e2-9d70-477a-8bdc-bce63e0ec1a5",
   "metadata": {},
   "source": [
    "### 5b. Vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9678f9c-cf04-4ec9-b1ba-f21ae18941a0",
   "metadata": {},
   "source": [
    "We will add the embeddings and the document chunks to an in-memory vector database. \n",
    "\n",
    "Two options are presented below. Both Milvus and Chroma are vector databases that can operate in-memory to provide fast, low-latency similarity searches for AI applications. Milvus is built for very large, complex projects that need to handle huge amounts of data, while Open Source Chroma is designed to be very simple and easy to start with for smaller, single-computer applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb744cb3-92d2-4b15-8f26-50322a4d43b9",
   "metadata": {},
   "source": [
    "#### Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "804775e7-a081-40dd-82c8-feec895b18d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/venv/lib64/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "2025-06-30 19:33:35,543 [DEBUG][_create_connection]: Created new connection using: d3a9a0edb25e48828e43713908b9847d (async_milvus_client.py:599)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of entries in the vector store is: 38\n"
     ]
    }
   ],
   "source": [
    "from langchain_milvus import Milvus\n",
    "\n",
    "vectorstore = Milvus.from_documents(  \n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    connection_args={\n",
    "        \"uri\": \"./milvus_demo.db\",\n",
    "    },\n",
    "    drop_old=True,  # Drop the old Milvus collection if it exists\n",
    "    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
    "\n",
    ")\n",
    "\n",
    "#check the number of entries corresponds with the number of sub-documents (chunks)\n",
    "vectorstore.col.flush()\n",
    "number_of_entries = vectorstore.col.num_entities\n",
    "print(f\"The number of entries in the vector store is: {number_of_entries}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d4059-fb88-4b90-b82c-c20fe641bd92",
   "metadata": {},
   "source": [
    "#### About index parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538617e8-9374-4f2c-bba7-a28ef80bdb34",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "An index in a vector database is a data structure that organizes the vectors. \n",
    "FLAT and HNSW (Hierarchical Navigable Small World) are two different types of indexes used in vector databases to manage and search through high-dimensional data. They represent a fundamental trade-off between search accuracy and speed.\n",
    "\n",
    "**FLAT Index** \n",
    "\n",
    "A FLAT index is the most basic approach to vector search. It is a brute-force method where a query vector is directly compared to every single other vector in the dataset. It is slow, but 100% accurate.\n",
    "In our example above the Milvus instance uses a FLAT index. We can change this parameter setting to HNSW.\n",
    "\n",
    "\n",
    "**HNSW (Hierarchical Navigable Small World) Index** \n",
    "\n",
    "HNSW is a sophisticated graph-based index that provides a powerful balance between search speed and accuracy. It's an Approximate Nearest Neighbor (ANN) algorithm, meaning it finds results that are most likely the nearest neighbors, but without a 100% guarantee.\n",
    "In our example above the Chroma instance uses a HNSW index.\n",
    "\n",
    "**Similarity Measures**\n",
    "\n",
    "In both cases we have specified L2 as an index parameter.\n",
    "\n",
    "This parameter defines how similarity between two vector embeddings is measured. \n",
    "L2 signifies that similarity between 2 vectors is measured by the shortest Euclidean distance between them in multi-dimensional space. \n",
    "\n",
    "In simpler terms, the vector embeddings can be thought of as lines in space. Two vector embeddings are 'similar' if the lines are close to each other. \n",
    "\n",
    "This is how our RAG system can identify the most relevant document chunks to feed to the LLM. The user query vector embedding is compared to the sub-document embeddings.\n",
    "\n",
    "L2 distance is not the only way to compare 2 vector embeddings. Another common choice is Cosine Similarity which measures the angle between 2 vectors in multi-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0209b9-9c65-4c1d-b4bb-cf47c220e09f",
   "metadata": {},
   "source": [
    "## 6. Build the RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad5c4f-87b0-42a3-ba08-3065216341a2",
   "metadata": {},
   "source": [
    "Now our documents have been chunked and stored in a vector database with their embeddings we are ready to define the components needed for our RAG system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48761b31-8587-45b3-aefa-2fc79bb887b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for this section\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a589b6f-36d0-4552-820e-929eec1f2485",
   "metadata": {},
   "source": [
    "#### Initialize the model\n",
    "\n",
    "Note the low temperature parameter which will give us accurate, factual responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e365ede6-b2d9-43d4-95a7-2b723c5b46bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "llm = ChatOpenAI(model=model, api_key=api_key, base_url=base_url, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d9b3b-e67c-40da-91c1-c1d0b84e9b3f",
   "metadata": {},
   "source": [
    "#### Create a retriever object\n",
    "The vectorstore.as_retriever() method creates a retriever object, which acts as a specialized search interface for the vector database.\n",
    "\n",
    "This retriever takes a query, uses the embedding model to get the query embedding vector, uses the vector store's index to efficiently find the most semantically relevant documents by vector comparison, and returns the top k documents, ready to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2baf7479-b4ca-4d10-ae83-da0acbf42512",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bc748-b6fd-431d-8d44-0b631cf1e13a",
   "metadata": {},
   "source": [
    "#### Define the prompt template\n",
    "\n",
    "Note that we are using PromptTemplate and manually specifying the roles Human and Assistant (see section 3c)\n",
    "You can also see two placeholders. `{question}` will be filled by the user's query and `{context}` will be filled by the article chunks returned by the retriever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae93160a-8052-481b-be83-2a21331b5aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Human: You are an AI assistant, and provide answers to questions by using fact based and statistical information when possible.\n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. Specify if the answer is in the context or not.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "The response should be specific and use statistics or numbers when possible.\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbfd32-cc70-4f13-9342-f132bd0f0d2f",
   "metadata": {},
   "source": [
    "#### Building the RAG chain\n",
    "The chain's first step is a dictionary with two keys, context and question. \n",
    "* \"context\" key: The input query is passed to the retriever, which fetches relevant documents. These documents are then piped to format_docs to be converted into a single string.\n",
    "* \"question\" key: The RunnablePassthrough() also receives the exact same input query. Its only job is to do nothing to it and pass it straight through.\n",
    "\n",
    "This output is then fed to the prompt, where the `context` and `question` placeholders are substituted by the retrieved document string and the user input query.\n",
    "\n",
    "The formatted prompt is passed to the LLM, and lastly the response is passed through an output parser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "377b4696-235e-4514-b102-f1ac94ac56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joins the returned documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "# builds the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f0bbe-c078-4a3f-b7fa-93d64cf1ccd6",
   "metadata": {},
   "source": [
    "## 7. Query and test the RAG system\n",
    "We can now invoke the RAG chain and test if it replies correctly to our queries with responses based on our stored documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82675e0c-ebc3-4904-8bff-34b36c7ccbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "Question :  What are the advantages of few shot prompting?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Response :  Few-shot prompting offers several advantages in AI model performance, particularly with models like GPT. Here are some key benefits:\n",
      "\n",
      "1. **Improved Contextual Understanding**: By providing a few examples related to the task at hand, few-shot prompting helps AI models understand the context better. This leads to more relevant and accurate responses compared to zero-shot prompting where no examples are given.\n",
      "\n",
      "2. **Generalization Capability**: Few-shot learning allows models to generalize from a small set of examples to new, unseen situations. A study by Lake et al. (2017) demonstrated that few-shot learning models could classify images into categories they had not seen during training with just 1, 3, or 5 examples per class, showing the model's ability to generalize effectively.\n",
      "\n",
      "3. **Reduced Need for Large Datasets**: Traditional machine learning often requires massive datasets for training. Few-shot prompting significantly reduces this requirement by enabling models to learn from a handful of examples, making it more efficient and cost-effective.\n",
      "\n",
      "4. **Adaptability to New Tasks**: With few-shot prompting, AI models can adapt to new tasks or domains with minimal additional data. This flexibility is crucial for real-world applications where the model might encounter diverse tasks without prior explicit programming for each one.\n",
      "\n",
      "Despite these advantages, it's important to note that few-shot learning still faces limitations, especially with highly complex tasks requiring deep reasoning or specialized knowledge, as mentioned in the context. The effectiveness can vary based on the task complexity and the model’s pre-training quality.\n",
      "\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the advantages of few shot prompting?\"\n",
    "\n",
    "res = rag_chain.invoke(query)\n",
    "print(\"--------------------------\\n\")\n",
    "print(\"Question : \",query)\n",
    "print(\"\\n--------------------------\\n\")\n",
    "print(\"Response : \",res)\n",
    "print(\"\\n--------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa060d-e216-40e1-9943-4e95d20abaa7",
   "metadata": {},
   "source": [
    "#### Explore the retrieved documents for comparison with the response\n",
    "We can invoke the retriever directly to explore the documents that have been used to inform the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a98c991-7f75-4ae1-9394-97584110b45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retrieved Documents ---\n",
      "\n",
      "--- Document 1 ---\n",
      "\n",
      "Source: https://medium.com/@tuhinsharma121/mastering-prompt-engineering-a-beginners-guide-to-ai-interaction-2a28434ccb67\n",
      "\n",
      "Content: is:Output:When we won the game, we all started to farduddle in celebration.LimitationStandard few-shot prompting is good for many tasks, but it’s not perfect, especially for complex thinking tasks. Let’s show why that is.Prompt:The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.A: The answer is False.The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.A: The answer is True.The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.A: The answer is True.The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.A: The answer is False.The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. A:Output:The answer is True.3. Chain-of-Thought (CoT) PromptingChain-of-thought (CoT) prompting helps machines think through problems step-by-step to understand them better. You can use it with few-shot prompting, where the machine sees a few examples first, to improve how it handles more difficult tasks that need thinking before answering.ExamplePrompt:The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. A:Output:Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.3.a. Zero-shot CoT PromptingLet’s try a simple problem and see how the model performs:Prompt:I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?Output:11 applesThe answer is incorrect! Now Let’s try with the special prompt “Let’s think step by step.”.ExamplePrompt:I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?Let's think step by\n",
      "\n",
      "\n",
      "--- Document 2 ---\n",
      "\n",
      "Source: https://medium.com/@tuhinsharma121/mastering-prompt-engineering-a-beginners-guide-to-ai-interaction-2a28434ccb67\n",
      "\n",
      "Content: AI models, as it showcases their ability to handle a wide variety of tasks and questions they have not explicitly been prepared for.ExamplePrompt:Classify the text into neutral, negative or positive. Text: I think the vacation is okay.Sentiment:Output:NeutralLimitationFirstly, these models often aren’t as accurate as models trained on specific tasks because they’re trying to generalize without direct examples. This can lead to errors or lower confidence in the results. Also, because they handle such a broad range of tasks, zero-shot models might struggle with very detailed or niche requests, where specialized knowledge is needed.Prompt:An example of a sentence that uses the word farduddle is:Output:The word \"farduddle\" doesn't appear to be a standard or recognized English word. Could it be a typo or a specific term from a particular context or language that you're exploring? If you have more details or a different spelling, please share, and I'll be happy to help with a sentence!2. Few Shot PromptingFew-shot prompting is a technique used in the field of artificial intelligence, particularly with models like GPT (Generative Pre-trained Transformer), to improve the model’s ability to understand and generate context-specific responses with minimal input data. Unlike zero-shot or one-shot methods, few-shot prompting involves providing the AI with a few examples to guide its responses, thereby helping the model infer the desired task from these examples without explicit programming.ExamplePrompt:A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:We were traveling in Africa and we saw these very cute whatpus. To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:Output:When we won the game, we all started to farduddle in celebration.LimitationStandard few-shot prompting is good for many tasks, but it’s not perfect, especially for complex thinking\n",
      "\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. Invoke the retriever directly to get the relevant documents\n",
    "\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "# 3. Print the retrieved documents to inspect their content\n",
    "print(\"--- Retrieved Documents ---\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Document {i+1} ---\\n\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\\n\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db48ffa-4a68-4c54-98ea-70eda2e85a3b",
   "metadata": {},
   "source": [
    "### Exploring and testing the RAG system\n",
    "Let's see what happens if we ask something that is not in the Meduim article documents. Is this the behaviour you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e06f1253-9b28-4175-be0f-41f38da0e962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "Question :  What is the capital of France?\n",
      "\n",
      "--------------------------\n",
      "\n",
      "Response :  I don't have real-time data or specific information about the capital of France. However, according to general knowledge, Paris is the capital city of France. It's not only the capital but also the most populous city in the country with approximately 2.1 million people within its metropolitan area as per the 2020 estimates.\n",
      "\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "query_test = \"What is the capital of France?\"\n",
    "\n",
    "res = rag_chain.invoke(query_test)\n",
    "print(\"--------------------------\\n\")\n",
    "print(\"Question : \",query_test)\n",
    "print(\"\\n--------------------------\\n\")\n",
    "print(\"Response : \",res)\n",
    "print(\"\\n--------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecea8fa7-ec06-44a5-9466-b29c3da045ce",
   "metadata": {},
   "source": [
    "#### Again explore the retrieved documents for comparison with the response\n",
    "Can you see any information related to the query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba22654f-d21e-4490-adfb-e48e121a8d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retrieved Documents ---\n",
      "\n",
      "--- Document 1 ---\n",
      "\n",
      "Source: https://medium.com/@fassha08/transforming-search-ai-agents-and-multi-vector-intelligence-1bde1dbe66e7\n",
      "\n",
      "Content: langchain_core.output_parsers import StrOutputParserfrom semantic_router import Routefrom semantic_router.encoders import HuggingFaceEncoderfrom semantic_router.layer import RouteLayerfrom langchain_community.tools.tavily_search import TavilySearchResultsimport osInstantiate the Embedding Modelembeddings = HuggingFaceEmbeddings(            model_name=\"BAAI/bge-large-en-v1.5\")Vector DatabasesIn this blog, we are using LangChain wrappers to connect to different vector databases. For more information on setting up PgVector, please refer to the PgVector documentation. Similarly, for Qdrant, you can refer to the Qdrant documentation. Regardless of which vector store we are using, the process remains the same.Data UsedDataset1: Technical documentationDomain: Software DevelopmentVector Database: PGVectorDataset2: Customer Service FAQsDomain: E-commerceVector Database: QdrntSetup PgVector DbDocuments can be loaded from the local hard disk or from web URLs, based on the requirement.#from localloader = DirectoryLoader('./data/pg/', glob=\"./*.txt\") documents = loader.load()#from web just for example purpose, not used in the blog urls = [    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",]docs = [WebBaseLoader(url).load() for url in urls]documents = [item for sublist in docs for item in sublist]print(f\"len of documents :{len(documents)}\")Create chunkstext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)docs = text_splitter.split_documents(documents)Lets see how are software domain data chunks looks likeAdd the documents to Pg vector#Cloud hosted Pg vector databaseconnection = \"postgresql://url here\"  collection_name = \"pg_data\"pg_vector_store = PGVector(        connection=connection,         collection_name=collection_name,         embeddings=embeddings    )pg_vector_store.add_documents(docs)Instantiate\n",
      "\n",
      "\n",
      "--- Document 2 ---\n",
      "\n",
      "Source: https://medium.com/@harshkumar1146/prompt-chaining-unlocking-the-full-potential-of-ai-assistants-4fdf2f28c1a5\n",
      "\n",
      "Content: key attractions, accommodations, and transportation.Step 1: Destination OverviewPrompt 1:I want to plan a 7-day trip to Germany. Can you suggest an itinerary?Output:Sure! Here's a suggested 7-day itinerary for Germany:Day 1-2: Berlin - Explore historical sites like the Brandenburg Gate,          Berlin Wall, and Museum Island.Day 3:   Dresden - Visit the Frauenkirche and Zwinger Palace.Day 4-5: Munich - Enjoy Marienplatz, Nymphenburg Palace, and the famous          beer gardens.Day 6:   Neuschwanstein Castle - Take a day trip to this fairy-tale castle.Day 7:   Frankfurt - Tour the Römer, Städel Museum, and Main Tower.Step 2: Break Down the Trip by DayPrompt:Great! Now, let's break down the itinerary day by day. What should I do onthe first day in Berlin?Output:On the first day in Berlin, you can:1. Morning: Visit the Brandenburg Gate and Reichstag Building.2. Afternoon: Explore the Berlin Wall Memorial and Checkpoint Charlie.3. Evening: Stroll along Unter den Linden and have dinner at a local   restaurant.Step 3: Add Details and PreferencesPrompt:I love German cuisine. Can you recommend some must-try restaurants in Berlin for day one? [Output from step 2]Output:Sure! Here are some must-try restaurants in Berlin:1. Frühstück 3000 for a unique breakfast experience.2. Zur Letzten Instanz, Berlin's oldest restaurant, for traditional    German dishes.3. Curry 36 for the famous currywurst.Step 4: Incorporate LogisticsPrompt:How should I travel from Berlin to Dresden on the third day?[Output from step 1]Output:You can travel from Berlin to Dresden by train. It's a comfortable 2-hourjourney with frequent departures. Consider booking tickets in advance through Deutsche Bahn for the best rates.Step 5: Customize Based on FeedbackPrompt:I prefer outdoor activities. Are there any hiking spots near Munich that Ican visit on day five? [Output from step 1]Output:Absolutely! Near Munich, you can hike in the Bavarian Alps. A popular trailis the Partnach Gorge in\n",
      "\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. Invoke the retriever directly to get the relevant documents\n",
    "\n",
    "retrieved_docs = retriever.invoke(query_test)\n",
    "\n",
    "# 3. Print the retrieved documents to inspect their content\n",
    "print(\"--- Retrieved Documents ---\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Document {i+1} ---\\n\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\\n\")\n",
    "    print(f\"Content: {doc.page_content}\\n\")\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13518b62-f001-4c32-a100-6816d6736227",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "You have successfully built a simple **Retrieval Augmented Generation (RAG) question** and answer system based on publicly available Medium articles.\n",
    "\n",
    "Well done!\n",
    "\n",
    "#### To go further\n",
    "- Experiment with different parameters and prompts\n",
    "- Try other models from models.corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff47182-c364-4e92-a350-0002b1acb6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
